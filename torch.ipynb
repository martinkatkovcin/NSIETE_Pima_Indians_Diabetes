{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhrnciar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(torch.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6    148.0           72.0           35.0    169.5  33.6   \n",
       "1              1     85.0           66.0           29.0    102.5  26.6   \n",
       "2              8    183.0           64.0           32.0    169.5  23.3   \n",
       "3              1     89.0           66.0           23.0     94.0  28.1   \n",
       "4              0    137.0           40.0           35.0    168.0  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10    101.0           76.0           48.0    180.0  32.9   \n",
       "764            2    122.0           70.0           27.0    102.5  36.8   \n",
       "765            5    121.0           72.0           23.0    112.0  26.2   \n",
       "766            1    126.0           60.0           32.0    169.5  30.1   \n",
       "767            1     93.0           70.0           31.0    102.5  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/cleaned.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.314928</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.171779</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.104294</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.185096</td>\n",
       "      <td>0.509202</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.367742</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.445652</td>\n",
       "      <td>0.199519</td>\n",
       "      <td>0.300613</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.503226</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.380368</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.496774</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.117788</td>\n",
       "      <td>0.163599</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.529032</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.243354</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.316129</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.249489</td>\n",
       "      <td>0.101196</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0       0.352941  0.670968       0.489796       0.304348  0.186899  0.314928   \n",
       "1       0.058824  0.264516       0.428571       0.239130  0.106370  0.171779   \n",
       "2       0.470588  0.896774       0.408163       0.271739  0.186899  0.104294   \n",
       "3       0.058824  0.290323       0.428571       0.173913  0.096154  0.202454   \n",
       "4       0.000000  0.600000       0.163265       0.304348  0.185096  0.509202   \n",
       "..           ...       ...            ...            ...       ...       ...   \n",
       "763     0.588235  0.367742       0.530612       0.445652  0.199519  0.300613   \n",
       "764     0.117647  0.503226       0.469388       0.217391  0.106370  0.380368   \n",
       "765     0.294118  0.496774       0.489796       0.173913  0.117788  0.163599   \n",
       "766     0.058824  0.529032       0.367347       0.271739  0.186899  0.243354   \n",
       "767     0.058824  0.316129       0.469388       0.260870  0.106370  0.249489   \n",
       "\n",
       "     DiabetesPedigreeFunction       Age  Outcome  \n",
       "0                    0.234415  0.483333        1  \n",
       "1                    0.116567  0.166667        0  \n",
       "2                    0.253629  0.183333        1  \n",
       "3                    0.038002  0.000000        0  \n",
       "4                    0.943638  0.200000        1  \n",
       "..                        ...       ...      ...  \n",
       "763                  0.039710  0.700000        0  \n",
       "764                  0.111870  0.100000        0  \n",
       "765                  0.071307  0.150000        0  \n",
       "766                  0.115713  0.433333        1  \n",
       "767                  0.101196  0.033333        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']] = minmax_scaling(df,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('Outcome', axis=1).values, df.Outcome.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=42)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "y_test = torch.FloatTensor(y_test).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (f_connected1): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (f_connected2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (out): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mhrnciar/Desktop/School/FIIT/Ing/2. Semester/Neurónové siete/Cvičenia/basic-nn/wandb/run-20230324_080455-vivid-serenity-15</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-15' target=\"_blank\">vivid-serenity-15</a></strong> to <a href='https://wandb.ai/mhrnciar/basic-nn-torch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mhrnciar/basic-nn-torch' target=\"_blank\">https://wandb.ai/mhrnciar/basic-nn-torch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-15' target=\"_blank\">https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-15</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=8, hidden2=4, out_features=1):\n",
    "        super().__init__()\n",
    "        self.f_connected1 = nn.Linear(input_features, hidden1)\n",
    "        self.f_connected2 = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.f_connected1(x))\n",
    "        x = self.relu(self.f_connected2(x))\n",
    "        x = self.sigmoid(self.out(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "config = {'lr': 0.001, 'epochs': 500}\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=1e-2, # Upper learning rate boundaries in the cycle for each parameter group\n",
    "                                                steps_per_epoch=1, # The number of steps per epoch to train for.\n",
    "                                                epochs=config['epochs'], # The number of epochs to train for.\n",
    "                                                anneal_strategy='cos') # Specifies the annealing strategy\n",
    "\n",
    "run = wandb.init(project=\"basic-nn-torch\", id=\"vivid-serenity-15\")\n",
    "wandb.config.update(config)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "----------------------------------------\n",
      "Training loss: 0.690196692943573, validation loss: 0.6911450624465942\n",
      "\n",
      "Epoch 20\n",
      "----------------------------------------\n",
      "Training loss: 0.6872970461845398, validation loss: 0.6882309317588806\n",
      "\n",
      "Epoch 30\n",
      "----------------------------------------\n",
      "Training loss: 0.6830071806907654, validation loss: 0.6839324831962585\n",
      "\n",
      "Epoch 40\n",
      "----------------------------------------\n",
      "Training loss: 0.676983118057251, validation loss: 0.6781080961227417\n",
      "\n",
      "Epoch 50\n",
      "----------------------------------------\n",
      "Training loss: 0.6696858406066895, validation loss: 0.6712237000465393\n",
      "\n",
      "Epoch 60\n",
      "----------------------------------------\n",
      "Training loss: 0.6623578667640686, validation loss: 0.6641794443130493\n",
      "\n",
      "Epoch 70\n",
      "----------------------------------------\n",
      "Training loss: 0.6520755290985107, validation loss: 0.654560387134552\n",
      "\n",
      "Epoch 80\n",
      "----------------------------------------\n",
      "Training loss: 0.6346878409385681, validation loss: 0.6378908753395081\n",
      "\n",
      "Epoch 90\n",
      "----------------------------------------\n",
      "Training loss: 0.6054604053497314, validation loss: 0.6099921464920044\n",
      "\n",
      "Epoch 100\n",
      "----------------------------------------\n",
      "Training loss: 0.5555819272994995, validation loss: 0.5637877583503723\n",
      "\n",
      "Epoch 110\n",
      "----------------------------------------\n",
      "Training loss: 0.4932020306587219, validation loss: 0.5118523240089417\n",
      "\n",
      "Epoch 120\n",
      "----------------------------------------\n",
      "Training loss: 0.44818034768104553, validation loss: 0.47300389409065247\n",
      "\n",
      "Epoch 130\n",
      "----------------------------------------\n",
      "Training loss: 0.4327031672000885, validation loss: 0.45452162623405457\n",
      "\n",
      "Epoch 140\n",
      "----------------------------------------\n",
      "Training loss: 0.4183216392993927, validation loss: 0.4467819929122925\n",
      "\n",
      "Epoch 150\n",
      "----------------------------------------\n",
      "Training loss: 0.4061885178089142, validation loss: 0.44081467390060425\n",
      "\n",
      "Epoch 160\n",
      "----------------------------------------\n",
      "Training loss: 0.3923082947731018, validation loss: 0.42686501145362854\n",
      "\n",
      "Epoch 170\n",
      "----------------------------------------\n",
      "Training loss: 0.37878096103668213, validation loss: 0.4069502651691437\n",
      "\n",
      "Epoch 180\n",
      "----------------------------------------\n",
      "Training loss: 0.36902984976768494, validation loss: 0.4023610055446625\n",
      "\n",
      "Epoch 190\n",
      "----------------------------------------\n",
      "Training loss: 0.36169731616973877, validation loss: 0.39167463779449463\n",
      "\n",
      "Epoch 200\n",
      "----------------------------------------\n",
      "Training loss: 0.3560017943382263, validation loss: 0.395648717880249\n",
      "\n",
      "Epoch 210\n",
      "----------------------------------------\n",
      "Training loss: 0.35122963786125183, validation loss: 0.3928261995315552\n",
      "\n",
      "Epoch 220\n",
      "----------------------------------------\n",
      "Training loss: 0.34680813550949097, validation loss: 0.39314571022987366\n",
      "\n",
      "Epoch 230\n",
      "----------------------------------------\n",
      "Training loss: 0.34279853105545044, validation loss: 0.39397943019866943\n",
      "\n",
      "Epoch 240\n",
      "----------------------------------------\n",
      "Training loss: 0.3396736979484558, validation loss: 0.39600709080696106\n",
      "\n",
      "Epoch 250\n",
      "----------------------------------------\n",
      "Training loss: 0.33701929450035095, validation loss: 0.39774107933044434\n",
      "\n",
      "Epoch 260\n",
      "----------------------------------------\n",
      "Training loss: 0.33475857973098755, validation loss: 0.40143251419067383\n",
      "\n",
      "Epoch 270\n",
      "----------------------------------------\n",
      "Training loss: 0.3328286111354828, validation loss: 0.4023666977882385\n",
      "\n",
      "Epoch 280\n",
      "----------------------------------------\n",
      "Training loss: 0.3306291699409485, validation loss: 0.40153688192367554\n",
      "\n",
      "Epoch 290\n",
      "----------------------------------------\n",
      "Training loss: 0.32881951332092285, validation loss: 0.3988564610481262\n",
      "\n",
      "Epoch 300\n",
      "----------------------------------------\n",
      "Training loss: 0.3272441029548645, validation loss: 0.40230792760849\n",
      "\n",
      "Epoch 310\n",
      "----------------------------------------\n",
      "Training loss: 0.3257080912590027, validation loss: 0.4027683436870575\n",
      "\n",
      "Epoch 320\n",
      "----------------------------------------\n",
      "Training loss: 0.3243859112262726, validation loss: 0.40250346064567566\n",
      "\n",
      "Epoch 330\n",
      "----------------------------------------\n",
      "Training loss: 0.3232439458370209, validation loss: 0.40434008836746216\n",
      "\n",
      "Epoch 340\n",
      "----------------------------------------\n",
      "Training loss: 0.3221917748451233, validation loss: 0.4047505855560303\n",
      "\n",
      "Epoch 350\n",
      "----------------------------------------\n",
      "Training loss: 0.3212721645832062, validation loss: 0.40477806329727173\n",
      "\n",
      "Epoch 360\n",
      "----------------------------------------\n",
      "Training loss: 0.32051992416381836, validation loss: 0.40629658102989197\n",
      "\n",
      "Epoch 370\n",
      "----------------------------------------\n",
      "Training loss: 0.31986841559410095, validation loss: 0.40713539719581604\n",
      "\n",
      "Epoch 380\n",
      "----------------------------------------\n",
      "Training loss: 0.31934887170791626, validation loss: 0.40697091817855835\n",
      "\n",
      "Epoch 390\n",
      "----------------------------------------\n",
      "Training loss: 0.31886163353919983, validation loss: 0.4067372977733612\n",
      "\n",
      "Epoch 400\n",
      "----------------------------------------\n",
      "Training loss: 0.318471759557724, validation loss: 0.40648967027664185\n",
      "\n",
      "Epoch 410\n",
      "----------------------------------------\n",
      "Training loss: 0.3181604743003845, validation loss: 0.4062436819076538\n",
      "\n",
      "Epoch 420\n",
      "----------------------------------------\n",
      "Training loss: 0.31792423129081726, validation loss: 0.4059903919696808\n",
      "\n",
      "Epoch 430\n",
      "----------------------------------------\n",
      "Training loss: 0.31774643063545227, validation loss: 0.4059200882911682\n",
      "\n",
      "Epoch 440\n",
      "----------------------------------------\n",
      "Training loss: 0.31761229038238525, validation loss: 0.4057767689228058\n",
      "\n",
      "Epoch 450\n",
      "----------------------------------------\n",
      "Training loss: 0.31751570105552673, validation loss: 0.40574732422828674\n",
      "\n",
      "Epoch 460\n",
      "----------------------------------------\n",
      "Training loss: 0.31744956970214844, validation loss: 0.4057973325252533\n",
      "\n",
      "Epoch 470\n",
      "----------------------------------------\n",
      "Training loss: 0.31740862131118774, validation loss: 0.40582650899887085\n",
      "\n",
      "Epoch 480\n",
      "----------------------------------------\n",
      "Training loss: 0.31738752126693726, validation loss: 0.4058389961719513\n",
      "\n",
      "Epoch 490\n",
      "----------------------------------------\n",
      "Training loss: 0.3173791766166687, validation loss: 0.4058440625667572\n",
      "\n",
      "Epoch 500\n",
      "----------------------------------------\n",
      "Training loss: 0.31737783551216125, validation loss: 0.40584656596183777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "\n",
    "for i in range(config['epochs']):\n",
    "    i += 1\n",
    "    y_pred = model.forward(X_train)\n",
    "    train_loss = loss_fn(y_pred, y_train)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    y_pred = (y_pred > 0.5).int()\n",
    "    train_accuracy = accuracy_score(y_train.squeeze(1).int(), y_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "    scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        val_loss = loss_fn(y_pred, y_test)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        y_pred = (y_pred > 0.5).int()\n",
    "        wandb.log({'training_loss': train_loss, 'validation_loss': val_loss}, commit=False)\n",
    "\n",
    "        f1_none = f1_score(y_test.squeeze(1).int(), y_pred, average=None)\n",
    "        f1_none = {'f1_none/' + str(e): v for e,v in enumerate(f1_none)}\n",
    "        wandb.log(f1_none, commit=False)\n",
    "\n",
    "        f1_macro = f1_score(y_test.squeeze(1).int(), y_pred, average='macro')\n",
    "        wandb.log({'f1_macro': f1_macro}, commit=False)\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_test.squeeze(1).int(), y_pred)\n",
    "        wandb.log({'train_accuracy': train_accuracy, 'val_accuracy': val_accuracy})\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}')\n",
    "        print('-' * 40)\n",
    "        print(f'Training loss: {train_loss}, validation loss: {val_loss}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "train",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.692169725894928,
          0.6919582486152649,
          0.69174724817276,
          0.6915354132652283,
          0.6913220882415771,
          0.6911062002182007,
          0.6908864974975586,
          0.6906623840332031,
          0.6904327273368835,
          0.690196692943573,
          0.6899531483650208,
          0.6897010207176208,
          0.689440131187439,
          0.6891691088676453,
          0.6888877749443054,
          0.6885949969291687,
          0.688289999961853,
          0.6879723072052002,
          0.6876416206359863,
          0.6872970461845398,
          0.686938464641571,
          0.6865651607513428,
          0.6861768364906311,
          0.6857730746269226,
          0.6853536367416382,
          0.6849183440208435,
          0.6844663619995117,
          0.6839971542358398,
          0.6835111379623413,
          0.6830071806907654,
          0.6824854612350464,
          0.6819458603858948,
          0.6813888549804688,
          0.6808139085769653,
          0.6802204847335815,
          0.6796090602874756,
          0.6789799928665161,
          0.6783320903778076,
          0.677666425704956,
          0.676983118057251,
          0.6762836575508118,
          0.6755678057670593,
          0.6748377084732056,
          0.6740955114364624,
          0.6733384728431702,
          0.6725819706916809,
          0.6718234419822693,
          0.671072244644165,
          0.6703699231147766,
          0.6696858406066895,
          0.6690102219581604,
          0.6683393716812134,
          0.6676697134971619,
          0.666983425617218,
          0.6662730574607849,
          0.6655356884002686,
          0.6647769212722778,
          0.6639976501464844,
          0.6631916761398315,
          0.6623578667640686,
          0.6614944338798523,
          0.660600483417511,
          0.6596751809120178,
          0.6587172746658325,
          0.6577239632606506,
          0.6566920280456543,
          0.6556161642074585,
          0.6544917821884155,
          0.653315544128418,
          0.6520755290985107,
          0.6507656574249268,
          0.6493886113166809,
          0.6479162573814392,
          0.6463354229927063,
          0.6446606516838074,
          0.6428956985473633,
          0.6410062313079834,
          0.6390049457550049,
          0.6368940472602844,
          0.6346878409385681,
          0.6323650479316711,
          0.6299343705177307,
          0.6273877024650574,
          0.624729335308075,
          0.6219375729560852,
          0.6190063953399658,
          0.6159107089042664,
          0.6126317977905273,
          0.6091573238372803,
          0.6054604053497314,
          0.6015234589576721,
          0.5973267555236816,
          0.5928512811660767,
          0.5881219506263733,
          0.5831888318061829,
          0.5781008005142212,
          0.5728291869163513,
          0.5673157572746277,
          0.5615547895431519,
          0.5555819272994995,
          0.5494799017906189,
          0.5432931184768677,
          0.5370262861251831,
          0.5307045578956604,
          0.5243582725524902,
          0.5180054306983948,
          0.5116718411445618,
          0.5054031610488892,
          0.4992240369319916,
          0.4932020306587219,
          0.4873700439929962,
          0.4817621111869812,
          0.4764200448989868,
          0.47135138511657715,
          0.46653613448143005,
          0.4620668888092041,
          0.45794612169265747,
          0.454267680644989,
          0.4510132670402527,
          0.44818034768104553,
          0.4457557499408722,
          0.44375595450401306,
          0.44193464517593384,
          0.4403463900089264,
          0.43892890214920044,
          0.4376184344291687,
          0.436372846364975,
          0.43511897325515747,
          0.4339151680469513,
          0.4327031672000885,
          0.43156030774116516,
          0.43027105927467346,
          0.428943395614624,
          0.42738455533981323,
          0.42572706937789917,
          0.42406123876571655,
          0.4225022494792938,
          0.42104291915893555,
          0.41965925693511963,
          0.4183216392993927,
          0.4169740378856659,
          0.415713369846344,
          0.41445598006248474,
          0.413324773311615,
          0.4121483564376831,
          0.41099944710731506,
          0.409829318523407,
          0.40865030884742737,
          0.40744057297706604,
          0.4061885178089142,
          0.4048495292663574,
          0.4035184383392334,
          0.40221408009529114,
          0.4008539319038391,
          0.39948058128356934,
          0.398100882768631,
          0.39666908979415894,
          0.39521917700767517,
          0.3937751054763794,
          0.3923082947731018,
          0.39089518785476685,
          0.3895097076892853,
          0.387979120016098,
          0.38651910424232483,
          0.3851887583732605,
          0.38392043113708496,
          0.3827208876609802,
          0.3815191686153412,
          0.38032475113868713,
          0.37878096103668213,
          0.37773698568344116,
          0.37700924277305603,
          0.3760504424571991,
          0.37456464767456055,
          0.37392520904541016,
          0.3732072114944458,
          0.37183505296707153,
          0.3714941143989563,
          0.370297908782959,
          0.36902984976768494,
          0.3686479330062866,
          0.36745014786720276,
          0.3664278984069824,
          0.3660334348678589,
          0.36492353677749634,
          0.3641014099121094,
          0.3636986017227173,
          0.3627767264842987,
          0.36196979880332947,
          0.36169731616973877,
          0.3613675832748413,
          0.36005330085754395,
          0.35964176058769226,
          0.35945671796798706,
          0.3584834933280945,
          0.3577953279018402,
          0.35766521096229553,
          0.35715922713279724,
          0.3562487065792084,
          0.3560017943382263,
          0.3556957244873047,
          0.35480332374572754,
          0.3546026051044464,
          0.35423433780670166,
          0.35337239503860474,
          0.35319340229034424,
          0.3527607321739197,
          0.3519918918609619,
          0.35178127884864807,
          0.35122963786125183,
          0.35068413615226746,
          0.3504314720630646,
          0.3498310148715973,
          0.3493811786174774,
          0.3490709960460663,
          0.34851500391960144,
          0.3481023907661438,
          0.3477705717086792,
          0.34722214937210083,
          0.34680813550949097,
          0.34646299481391907,
          0.34599193930625916,
          0.3455755114555359,
          0.3452298641204834,
          0.34474968910217285,
          0.34441033005714417,
          0.3439963459968567,
          0.34357500076293945,
          0.3432222902774811,
          0.34279853105545044,
          0.3425045311450958,
          0.34215980768203735,
          0.3418017029762268,
          0.34150803089141846,
          0.3411577045917511,
          0.3408627510070801,
          0.34055909514427185,
          0.34024757146835327,
          0.33998242020606995,
          0.3396736979484558,
          0.3393884301185608,
          0.339132159948349,
          0.33886972069740295,
          0.3385946750640869,
          0.33832377195358276,
          0.3380606770515442,
          0.337796688079834,
          0.3375277817249298,
          0.33725762367248535,
          0.33701929450035095,
          0.33675360679626465,
          0.33651605248451233,
          0.33627110719680786,
          0.3360522389411926,
          0.33581456542015076,
          0.33560463786125183,
          0.3353717029094696,
          0.3351755440235138,
          0.33494529128074646,
          0.33475857973098755,
          0.3345262408256531,
          0.33429455757141113,
          0.3341113030910492,
          0.3338882625102997,
          0.33370786905288696,
          0.33351704478263855,
          0.33332139253616333,
          0.33315807580947876,
          0.3329671025276184,
          0.3328286111354828,
          0.33265095949172974,
          0.3324854373931885,
          0.3322557210922241,
          0.3320208489894867,
          0.33177971839904785,
          0.33153387904167175,
          0.33130332827568054,
          0.3310633599758148,
          0.33084988594055176,
          0.3306291699409485,
          0.3304089605808258,
          0.33013269305229187,
          0.32987523078918457,
          0.3296426236629486,
          0.329477459192276,
          0.32933005690574646,
          0.3292106091976166,
          0.3290881812572479,
          0.328957736492157,
          0.32881951332092285,
          0.3286713659763336,
          0.32851532101631165,
          0.328352153301239,
          0.3281848132610321,
          0.328014612197876,
          0.32784926891326904,
          0.3277028799057007,
          0.3275645971298218,
          0.3274022340774536,
          0.3272441029548645,
          0.3270799517631531,
          0.32693567872047424,
          0.3267918825149536,
          0.3266506493091583,
          0.32649102807044983,
          0.32633960247039795,
          0.3261793851852417,
          0.3260257840156555,
          0.3258633315563202,
          0.3257080912590027,
          0.32557299733161926,
          0.32543209195137024,
          0.3253014385700226,
          0.3251631259918213,
          0.32503557205200195,
          0.3249048590660095,
          0.32477787137031555,
          0.3246452510356903,
          0.32451432943344116,
          0.3243859112262726,
          0.3242514431476593,
          0.32413116097450256,
          0.3240100145339966,
          0.32389214634895325,
          0.32378265261650085,
          0.32367220520973206,
          0.3235691785812378,
          0.3234689235687256,
          0.32335764169692993,
          0.3232439458370209,
          0.3231360614299774,
          0.32301634550094604,
          0.3229103088378906,
          0.3228112757205963,
          0.3227039873600006,
          0.3226063847541809,
          0.3224892318248749,
          0.322401762008667,
          0.3222920298576355,
          0.3221917748451233,
          0.3220987021923065,
          0.32199767231941223,
          0.32189470529556274,
          0.3218011260032654,
          0.32170724868774414,
          0.32161107659339905,
          0.32152247428894043,
          0.3214344382286072,
          0.32135123014450073,
          0.3212721645832062,
          0.3211909532546997,
          0.32111209630966187,
          0.32103410363197327,
          0.3209567368030548,
          0.3208814859390259,
          0.32080888748168945,
          0.32073691487312317,
          0.32066455483436584,
          0.3205930292606354,
          0.32051992416381836,
          0.32044973969459534,
          0.320380836725235,
          0.320313423871994,
          0.3202500343322754,
          0.3201824128627777,
          0.32011720538139343,
          0.32005107402801514,
          0.31998831033706665,
          0.3199297785758972,
          0.31986841559410095,
          0.31981149315834045,
          0.3197552561759949,
          0.319701224565506,
          0.31965652108192444,
          0.3196074068546295,
          0.31955987215042114,
          0.319511741399765,
          0.31945085525512695,
          0.31940069794654846,
          0.31934887170791626,
          0.31929588317871094,
          0.3192448914051056,
          0.319193035364151,
          0.31914377212524414,
          0.3190949261188507,
          0.31904658675193787,
          0.31899940967559814,
          0.3189522624015808,
          0.31890591979026794,
          0.31886163353919983,
          0.31881916522979736,
          0.3187766671180725,
          0.31873518228530884,
          0.318695068359375,
          0.31865713000297546,
          0.31861957907676697,
          0.3185822367668152,
          0.3185451328754425,
          0.3185082674026489,
          0.318471759557724,
          0.3184371292591095,
          0.31840255856513977,
          0.3183687627315521,
          0.31833556294441223,
          0.3183040916919708,
          0.3182730972766876,
          0.31824326515197754,
          0.3182142376899719,
          0.3181877136230469,
          0.3181604743003845,
          0.3181329369544983,
          0.31810635328292847,
          0.31808194518089294,
          0.3180580139160156,
          0.31803423166275024,
          0.31801074743270874,
          0.3179878890514374,
          0.3179660141468048,
          0.3179449737071991,
          0.31792423129081726,
          0.31790363788604736,
          0.317883163690567,
          0.3178632855415344,
          0.3178441822528839,
          0.31782788038253784,
          0.31781014800071716,
          0.31779131293296814,
          0.3177759051322937,
          0.31776124238967896,
          0.31774643063545227,
          0.3177310526371002,
          0.31771552562713623,
          0.317701518535614,
          0.31768861413002014,
          0.3176752030849457,
          0.31766143441200256,
          0.3176482021808624,
          0.31763648986816406,
          0.3176244795322418,
          0.31761229038238525,
          0.3176012635231018,
          0.3175903260707855,
          0.3175797164440155,
          0.31756967306137085,
          0.3175598084926605,
          0.3175503611564636,
          0.31754109263420105,
          0.31753233075141907,
          0.31752365827560425,
          0.31751570105552673,
          0.31750792264938354,
          0.31750017404556274,
          0.31749284267425537,
          0.3174857795238495,
          0.3174791932106018,
          0.31747278571128845,
          0.31746649742126465,
          0.3174605369567871,
          0.317454993724823,
          0.31744956970214844,
          0.3174442946910858,
          0.31743964552879333,
          0.3174351453781128,
          0.31743061542510986,
          0.3174263536930084,
          0.3174222707748413,
          0.31741863489151,
          0.31741514801979065,
          0.31741178035736084,
          0.31740862131118774,
          0.3174058198928833,
          0.317403107881546,
          0.31740066409111023,
          0.3173983097076416,
          0.31739622354507446,
          0.3173941373825073,
          0.3173923194408417,
          0.3173905611038208,
          0.3173889219760895,
          0.31738752126693726,
          0.3173861801624298,
          0.3173849880695343,
          0.31738391518592834,
          0.31738296151161194,
          0.3173820674419403,
          0.317381352186203,
          0.3173806667327881,
          0.31738007068634033,
          0.3173796236515045,
          0.3173791766166687,
          0.31737884879112244,
          0.31737855076789856,
          0.31737834215164185,
          0.3173781931400299,
          0.31737804412841797,
          0.3173779547214508,
          0.3173779547214508,
          0.31737789511680603,
          0.31737789511680603,
          0.31737783551216125
         ]
        },
        {
         "name": "validation",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.6930668950080872,
          0.6928647756576538,
          0.6926620602607727,
          0.6924578547477722,
          0.6922509670257568,
          0.6920405030250549,
          0.691825270652771,
          0.6916047930717468,
          0.6913784146308899,
          0.6911450624465942,
          0.6909027099609375,
          0.6906515955924988,
          0.6903902888298035,
          0.690118670463562,
          0.689835786819458,
          0.6895409226417542,
          0.6892331838607788,
          0.6889127492904663,
          0.6885795593261719,
          0.6882309317588806,
          0.6878693699836731,
          0.687493085861206,
          0.6871017217636108,
          0.6866951584815979,
          0.6862739324569702,
          0.6858369708061218,
          0.6853853464126587,
          0.6849179267883301,
          0.68443363904953,
          0.6839324831962585,
          0.6834152936935425,
          0.6828823089599609,
          0.6823344230651855,
          0.6817703247070312,
          0.6811903715133667,
          0.6805984973907471,
          0.6799936294555664,
          0.6793742775917053,
          0.6787487268447876,
          0.6781080961227417,
          0.6774526834487915,
          0.6767773628234863,
          0.6760924458503723,
          0.6754046082496643,
          0.6747162342071533,
          0.6739892959594727,
          0.6732487082481384,
          0.672549843788147,
          0.6718719005584717,
          0.6712237000465393,
          0.6705598831176758,
          0.6698941588401794,
          0.6692315340042114,
          0.6685596108436584,
          0.6678779721260071,
          0.6671770811080933,
          0.666460394859314,
          0.665722668170929,
          0.6649624705314636,
          0.6641794443130493,
          0.6633722186088562,
          0.6625381708145142,
          0.6616760492324829,
          0.6607815027236938,
          0.6598529815673828,
          0.6588857173919678,
          0.6578766703605652,
          0.6568253040313721,
          0.655718982219696,
          0.654560387134552,
          0.6533388495445251,
          0.652042806148529,
          0.6506507396697998,
          0.6491474509239197,
          0.647514283657074,
          0.6457736492156982,
          0.6439682841300964,
          0.6420609354972839,
          0.6400295495986938,
          0.6378908753395081,
          0.6356906890869141,
          0.6334224343299866,
          0.6310772895812988,
          0.6285586953163147,
          0.6259181499481201,
          0.6231197714805603,
          0.6201571226119995,
          0.6170004606246948,
          0.6136175990104675,
          0.6099921464920044,
          0.6061423420906067,
          0.6020442247390747,
          0.5977253317832947,
          0.5932297110557556,
          0.5885814428329468,
          0.5838073492050171,
          0.5789444446563721,
          0.5739554762840271,
          0.568882167339325,
          0.5637877583503723,
          0.5585578680038452,
          0.5533111095428467,
          0.5480362772941589,
          0.5427970886230469,
          0.5375121831893921,
          0.532249391078949,
          0.5270371437072754,
          0.5218847990036011,
          0.5168025493621826,
          0.5118523240089417,
          0.5070816874504089,
          0.5025455355644226,
          0.4980306327342987,
          0.49353283643722534,
          0.4892502725124359,
          0.48546576499938965,
          0.48228171467781067,
          0.47923681139945984,
          0.4761536717414856,
          0.47300389409065247,
          0.4702589809894562,
          0.4682774245738983,
          0.4670535624027252,
          0.4652421772480011,
          0.4630388617515564,
          0.4612916111946106,
          0.4600675404071808,
          0.4590938687324524,
          0.4566812515258789,
          0.45452162623405457,
          0.4540221393108368,
          0.45500099658966064,
          0.45304548740386963,
          0.4511401057243347,
          0.451052188873291,
          0.4499175250530243,
          0.44788166880607605,
          0.4469611346721649,
          0.4476717710494995,
          0.4467819929122925,
          0.44555363059043884,
          0.44525736570358276,
          0.4458017349243164,
          0.44404226541519165,
          0.44297483563423157,
          0.44321876764297485,
          0.4429740905761719,
          0.4417160749435425,
          0.44224703311920166,
          0.44081467390060425,
          0.43968456983566284,
          0.4391690492630005,
          0.43658873438835144,
          0.4357399344444275,
          0.43457451462745667,
          0.43331611156463623,
          0.4311029016971588,
          0.429986834526062,
          0.4268946945667267,
          0.42686501145362854,
          0.42246195673942566,
          0.4217168986797333,
          0.41818955540657043,
          0.41786596179008484,
          0.4130297601222992,
          0.4153609573841095,
          0.4091208279132843,
          0.41376522183418274,
          0.40840283036231995,
          0.4069502651691437,
          0.4114883840084076,
          0.4034390449523926,
          0.40645790100097656,
          0.4078676402568817,
          0.4000357687473297,
          0.4009805917739868,
          0.405931293964386,
          0.39740145206451416,
          0.3980514109134674,
          0.4023610055446625,
          0.39574873447418213,
          0.396048903465271,
          0.3999713063240051,
          0.3950653374195099,
          0.3952257037162781,
          0.39879110455513,
          0.39434823393821716,
          0.3948543965816498,
          0.3971945643424988,
          0.39167463779449463,
          0.39417046308517456,
          0.3948839008808136,
          0.3909892737865448,
          0.39458972215652466,
          0.39373499155044556,
          0.3910164535045624,
          0.3959311544895172,
          0.3924872875213623,
          0.3911399245262146,
          0.395648717880249,
          0.3921319544315338,
          0.39055392146110535,
          0.3949131965637207,
          0.39176347851753235,
          0.39011284708976746,
          0.39400461316108704,
          0.3915283679962158,
          0.39020344614982605,
          0.3934553563594818,
          0.3928261995315552,
          0.390816330909729,
          0.3935408890247345,
          0.39367419481277466,
          0.3918261229991913,
          0.3941028416156769,
          0.39441221952438354,
          0.39256197214126587,
          0.3943684995174408,
          0.3945799469947815,
          0.39314571022987366,
          0.3948257863521576,
          0.3947826623916626,
          0.3933489918708801,
          0.3945525288581848,
          0.3953856825828552,
          0.39379847049713135,
          0.39421403408050537,
          0.3955496549606323,
          0.39468759298324585,
          0.39397943019866943,
          0.3958281874656677,
          0.39557942748069763,
          0.39455267786979675,
          0.3956778943538666,
          0.3960724174976349,
          0.39521169662475586,
          0.39566028118133545,
          0.3965475261211395,
          0.39579999446868896,
          0.39600709080696106,
          0.39645394682884216,
          0.39588621258735657,
          0.3962959349155426,
          0.3965112268924713,
          0.39646512269973755,
          0.3972550332546234,
          0.3969078063964844,
          0.39727020263671875,
          0.3981908857822418,
          0.39774107933044434,
          0.39790529012680054,
          0.39869093894958496,
          0.3994519114494324,
          0.39929303526878357,
          0.3990843594074249,
          0.40004807710647583,
          0.4006423354148865,
          0.4002408981323242,
          0.3998398184776306,
          0.40143251419067383,
          0.4014194905757904,
          0.4007435441017151,
          0.4018402397632599,
          0.4021648168563843,
          0.40135639905929565,
          0.401589035987854,
          0.40252310037612915,
          0.40216922760009766,
          0.40156978368759155,
          0.4023666977882385,
          0.4025794565677643,
          0.40169355273246765,
          0.4015974700450897,
          0.4023665189743042,
          0.40233203768730164,
          0.4016801118850708,
          0.40206092596054077,
          0.40267178416252136,
          0.40191349387168884,
          0.40153688192367554,
          0.4014713764190674,
          0.4013015329837799,
          0.40076184272766113,
          0.40047550201416016,
          0.40019288659095764,
          0.3996693193912506,
          0.399160236120224,
          0.39900150895118713,
          0.39895594120025635,
          0.3988564610481262,
          0.3988286256790161,
          0.39902690052986145,
          0.3993995487689972,
          0.39975225925445557,
          0.4000769853591919,
          0.40069273114204407,
          0.40112680196762085,
          0.40090644359588623,
          0.4012153744697571,
          0.40230792760849,
          0.4028260409832001,
          0.402442991733551,
          0.4025561511516571,
          0.40325725078582764,
          0.4034348726272583,
          0.40283387899398804,
          0.40248924493789673,
          0.40259283781051636,
          0.4028247594833374,
          0.4027683436870575,
          0.4021645188331604,
          0.40180468559265137,
          0.4020358920097351,
          0.40232744812965393,
          0.4021648168563843,
          0.4018111526966095,
          0.401935338973999,
          0.40233808755874634,
          0.40263310074806213,
          0.40250346064567566,
          0.40224596858024597,
          0.4024285674095154,
          0.40287715196609497,
          0.4032265841960907,
          0.40322211384773254,
          0.40308722853660583,
          0.40304163098335266,
          0.4033735692501068,
          0.4039623737335205,
          0.40434008836746216,
          0.4041600525379181,
          0.4039243757724762,
          0.40420830249786377,
          0.4047237038612366,
          0.4049490690231323,
          0.40445366501808167,
          0.40388450026512146,
          0.40397992730140686,
          0.40449368953704834,
          0.4047505855560303,
          0.40474486351013184,
          0.40449291467666626,
          0.4041796624660492,
          0.4041035771369934,
          0.4043315052986145,
          0.4045315384864807,
          0.4046366512775421,
          0.40464505553245544,
          0.4046137034893036,
          0.40477806329727173,
          0.40499812364578247,
          0.4051969051361084,
          0.4053305387496948,
          0.40545767545700073,
          0.405530720949173,
          0.40561598539352417,
          0.4058077037334442,
          0.4060838222503662,
          0.4062443971633911,
          0.40629658102989197,
          0.4063822031021118,
          0.40653368830680847,
          0.4067045748233795,
          0.40698447823524475,
          0.4070814549922943,
          0.4070538580417633,
          0.40686628222465515,
          0.40680035948753357,
          0.4069805145263672,
          0.40713539719581604,
          0.40717175602912903,
          0.4070984721183777,
          0.4069955050945282,
          0.4069570302963257,
          0.40705206990242004,
          0.40710750222206116,
          0.4070105254650116,
          0.40686097741127014,
          0.40686923265457153,
          0.40697091817855835,
          0.4070923924446106,
          0.40709614753723145,
          0.4069989025592804,
          0.4068818688392639,
          0.4067738652229309,
          0.4066864252090454,
          0.40664926171302795,
          0.40667131543159485,
          0.4067152440547943,
          0.4067372977733612,
          0.40672165155410767,
          0.4066758155822754,
          0.40665504336357117,
          0.40664058923721313,
          0.4066462814807892,
          0.4066412150859833,
          0.40662121772766113,
          0.4065835773944855,
          0.4065370261669159,
          0.40648967027664185,
          0.406493604183197,
          0.40651485323905945,
          0.4065377414226532,
          0.4065498113632202,
          0.406504362821579,
          0.40642061829566956,
          0.40632402896881104,
          0.4062427580356598,
          0.4062185287475586,
          0.4062436819076538,
          0.40628722310066223,
          0.40630993247032166,
          0.40630215406417847,
          0.4062637388706207,
          0.4062020480632782,
          0.40612682700157166,
          0.406057208776474,
          0.4060068428516388,
          0.4059842526912689,
          0.4059903919696808,
          0.4060242474079132,
          0.40606987476348877,
          0.40611544251441956,
          0.4061502516269684,
          0.4061380922794342,
          0.4060828387737274,
          0.4060094356536865,
          0.4059516191482544,
          0.40592238306999207,
          0.4059200882911682,
          0.4059394896030426,
          0.405963271856308,
          0.40596383810043335,
          0.40593650937080383,
          0.40588605403900146,
          0.4058244526386261,
          0.4057841897010803,
          0.40576809644699097,
          0.4057718515396118,
          0.4057767689228058,
          0.40577489137649536,
          0.4057661294937134,
          0.4057662785053253,
          0.4057735502719879,
          0.4057771861553192,
          0.40576356649398804,
          0.40575188398361206,
          0.40574759244918823,
          0.40575024485588074,
          0.40574732422828674,
          0.40574008226394653,
          0.4057336747646332,
          0.40573880076408386,
          0.40575388073921204,
          0.405764102935791,
          0.4057694971561432,
          0.4057706892490387,
          0.4057762324810028,
          0.40578535199165344,
          0.4057973325252533,
          0.4058113396167755,
          0.40581998229026794,
          0.4058232009410858,
          0.40582171082496643,
          0.40581658482551575,
          0.40581434965133667,
          0.40581485629081726,
          0.40581759810447693,
          0.4058223366737366,
          0.40582650899887085,
          0.4058261811733246,
          0.40582603216171265,
          0.405827134847641,
          0.40582939982414246,
          0.40582990646362305,
          0.40583139657974243,
          0.4058334231376648,
          0.405835896730423,
          0.4058385193347931,
          0.4058389961719513,
          0.4058382511138916,
          0.40583810210227966,
          0.4058383107185364,
          0.4058387875556946,
          0.4058394432067871,
          0.40584030747413635,
          0.40584123134613037,
          0.4058421850204468,
          0.4058431386947632,
          0.4058440625667572,
          0.4058449864387512,
          0.405845582485199,
          0.4058459997177124,
          0.4058462679386139,
          0.40584641695022583,
          0.405846506357193,
          0.4058465361595154,
          0.4058464765548706,
          0.40584656596183777,
          0.40584656596183777
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=list(range(config['epochs'])), y=list(map(lambda x: x.item(), train_losses)), name='train')\n",
    "fig.add_scatter(x=list(range(config['epochs'])), y=list(map(lambda x: x.item(), val_losses)), name='validation')\n",
    "fig.update_layout(xaxis_title='Epoch', yaxis_title='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>▃▄▁▁▁▁▁▂▅▆▆▆▇▇▇█████████████████████████</td></tr><tr><td>f1_none/0</td><td>▁▄▅▆▆▆▆▆▆▆▆▆▇▇▇██████████████████▇▇▇▇▇▇▇</td></tr><tr><td>f1_none/1</td><td>▅▅▁▁▁▁▁▂▆▇▇▇▇███████████████████████████</td></tr><tr><td>learning_rate</td><td>▁▁▂▂▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▃▃▃▃▃▅▆▆▆▇▇▇▇▇▇██████████████████████</td></tr><tr><td>training_loss</td><td>█████▇▇▆▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▃▃▃▃▃▃▄▆▆▆▇▇▇▇██████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>████▇▇▇▆▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>0.8415</td></tr><tr><td>f1_none/0</td><td>0.87958</td></tr><tr><td>f1_none/1</td><td>0.80342</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_accuracy</td><td>0.86645</td></tr><tr><td>training_loss</td><td>0.31738</td></tr><tr><td>val_accuracy</td><td>0.85065</td></tr><tr><td>validation_loss</td><td>0.40585</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-serenity-15</strong> at: <a href='https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-15' target=\"_blank\">https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-15</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230324_080455-vivid-serenity-15/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/model.pth\")\n",
    "wandb.save('runs/pima_run_2023-03-22')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8506493506493507"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_pred = model(data)\n",
    "        predictions.append((y_pred > 0.5).int().item())\n",
    "\n",
    "score = accuracy_score(y_test, predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
