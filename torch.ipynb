{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "True\n",
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "\n",
    "import wandb\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6    148.0           72.0           35.0    169.5  33.6   \n",
       "1              1     85.0           66.0           29.0    102.5  26.6   \n",
       "2              8    183.0           64.0           32.0    169.5  23.3   \n",
       "3              1     89.0           66.0           23.0     94.0  28.1   \n",
       "4              0    137.0           40.0           35.0    168.0  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10    101.0           76.0           48.0    180.0  32.9   \n",
       "764            2    122.0           70.0           27.0    102.5  36.8   \n",
       "765            5    121.0           72.0           23.0    112.0  26.2   \n",
       "766            1    126.0           60.0           32.0    169.5  30.1   \n",
       "767            1     93.0           70.0           31.0    102.5  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/cleaned.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('Outcome', axis=1).values, df.Outcome.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (f_connected1): Linear(in_features=8, out_features=20, bias=True)\n",
      "  (f_connected2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (f_connected3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=10, hidden3= 5, out_features=2):\n",
    "        super().__init__()\n",
    "        self.f_connected1 = nn.Linear(input_features, hidden1)\n",
    "        self.f_connected2 = nn.Linear(hidden1, hidden2)\n",
    "        self.f_connected3 = nn.Linear(hidden2, hidden3)\n",
    "        self.out = nn.Linear(hidden3, out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.f_connected1(x))\n",
    "        x = F.relu(self.f_connected2(x))\n",
    "        x = F.relu(self.f_connected3(x))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 0.5775846838951111\n",
      "Epoch 20: 0.49366334080696106\n",
      "Epoch 30: 0.4600379168987274\n",
      "Epoch 40: 0.44300633668899536\n",
      "Epoch 50: 0.4259662330150604\n",
      "Epoch 60: 0.41066673398017883\n",
      "Epoch 70: 0.39965084195137024\n",
      "Epoch 80: 0.3958785831928253\n",
      "Epoch 90: 0.3857158124446869\n",
      "Epoch 100: 0.3738996684551239\n",
      "Epoch 110: 0.3661211133003235\n",
      "Epoch 120: 0.3612293303012848\n",
      "Epoch 130: 0.3559854328632355\n",
      "Epoch 140: 0.356245219707489\n",
      "Epoch 150: 0.35114189982414246\n",
      "Epoch 160: 0.3439518213272095\n",
      "Epoch 170: 0.33825650811195374\n",
      "Epoch 180: 0.3412216901779175\n",
      "Epoch 190: 0.33816710114479065\n",
      "Epoch 200: 0.3372369408607483\n",
      "Epoch 210: 0.32699310779571533\n",
      "Epoch 220: 0.32468509674072266\n",
      "Epoch 230: 0.32271113991737366\n",
      "Epoch 240: 0.32409554719924927\n",
      "Epoch 250: 0.3335336744785309\n",
      "Epoch 260: 0.323016494512558\n",
      "Epoch 270: 0.31942862272262573\n",
      "Epoch 280: 0.3175789415836334\n",
      "Epoch 290: 0.3228004574775696\n",
      "Epoch 300: 0.34253451228141785\n",
      "Epoch 310: 0.3170837163925171\n",
      "Epoch 320: 0.314464271068573\n",
      "Epoch 330: 0.3124334514141083\n",
      "Epoch 340: 0.31159457564353943\n",
      "Epoch 350: 0.33124831318855286\n",
      "Epoch 360: 0.3232109844684601\n",
      "Epoch 370: 0.31730523705482483\n",
      "Epoch 380: 0.30819520354270935\n",
      "Epoch 390: 0.31026825308799744\n",
      "Epoch 400: 0.3117285966873169\n",
      "Epoch 410: 0.3106936812400818\n",
      "Epoch 420: 0.3080722987651825\n",
      "Epoch 430: 0.30446237325668335\n",
      "Epoch 440: 0.3103790581226349\n",
      "Epoch 450: 0.31548455357551575\n",
      "Epoch 460: 0.3028930127620697\n",
      "Epoch 470: 0.30190610885620117\n",
      "Epoch 480: 0.3152773976325989\n",
      "Epoch 490: 0.31111031770706177\n",
      "Epoch 500: 0.3018512427806854\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "final_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    final_losses.append(loss)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}: {loss.item()}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1.5351858139038086,
          0.9011818766593933,
          0.6740310788154602,
          0.6117025017738342,
          0.6933926939964294,
          0.566275954246521,
          0.5796166062355042,
          0.5351913571357727,
          0.5317426919937134,
          0.5775846838951111,
          0.5505640506744385,
          0.5238563418388367,
          0.5279901623725891,
          0.5194021463394165,
          0.5088675022125244,
          0.5203070044517517,
          0.5040440559387207,
          0.4946894347667694,
          0.49654608964920044,
          0.49366334080696106,
          0.49490803480148315,
          0.4881896376609802,
          0.48566848039627075,
          0.47569820284843445,
          0.4714682102203369,
          0.4653702974319458,
          0.46746355295181274,
          0.46556365489959717,
          0.4648713767528534,
          0.4600379168987274,
          0.4571271240711212,
          0.45427587628364563,
          0.45462489128112793,
          0.45279327034950256,
          0.4516404867172241,
          0.44781696796417236,
          0.4463137686252594,
          0.44445836544036865,
          0.44409140944480896,
          0.44300633668899536,
          0.44101250171661377,
          0.43919944763183594,
          0.4364425837993622,
          0.4352324604988098,
          0.43430355191230774,
          0.4328162372112274,
          0.4311925172805786,
          0.4291170537471771,
          0.42714598774909973,
          0.4259662330150604,
          0.4245496690273285,
          0.42252853512763977,
          0.4207857549190521,
          0.41970503330230713,
          0.4185919165611267,
          0.4170340299606323,
          0.41523078083992004,
          0.4135838747024536,
          0.41209012269973755,
          0.41066673398017883,
          0.40907710790634155,
          0.4077675938606262,
          0.4067307412624359,
          0.4058111011981964,
          0.4053780734539032,
          0.40823128819465637,
          0.4225268065929413,
          0.45359036326408386,
          0.4238782823085785,
          0.39965084195137024,
          0.4232427179813385,
          0.4094330072402954,
          0.3971906006336212,
          0.4130639135837555,
          0.39849594235420227,
          0.3978284001350403,
          0.404784619808197,
          0.39179328083992004,
          0.3997868597507477,
          0.3958785831928253,
          0.3904234766960144,
          0.39728355407714844,
          0.38876020908355713,
          0.39078259468078613,
          0.3911300599575043,
          0.38518816232681274,
          0.3890867829322815,
          0.3852045238018036,
          0.38387006521224976,
          0.3857158124446869,
          0.3814057409763336,
          0.3823435604572296,
          0.3822985589504242,
          0.3787863552570343,
          0.3799101412296295,
          0.3790883719921112,
          0.376270592212677,
          0.3768930733203888,
          0.3764786124229431,
          0.3738996684551239,
          0.37341630458831787,
          0.3737541735172272,
          0.3720695972442627,
          0.3703905940055847,
          0.370292067527771,
          0.37026506662368774,
          0.36905041337013245,
          0.36738017201423645,
          0.36638766527175903,
          0.3661211133003235,
          0.3658781051635742,
          0.3653203547000885,
          0.36443349719047546,
          0.36351656913757324,
          0.36251792311668396,
          0.36165064573287964,
          0.360956609249115,
          0.3604840934276581,
          0.36035680770874023,
          0.3612293303012848,
          0.36454182863235474,
          0.37482255697250366,
          0.39427298307418823,
          0.4149770438671112,
          0.3806660771369934,
          0.3568708896636963,
          0.375316321849823,
          0.38540783524513245,
          0.3633664548397064,
          0.3559854328632355,
          0.37100744247436523,
          0.36479103565216064,
          0.3533008396625519,
          0.3651028871536255,
          0.36485955119132996,
          0.35211434960365295,
          0.3585302531719208,
          0.36104458570480347,
          0.3511708974838257,
          0.356245219707489,
          0.3581284284591675,
          0.3501533567905426,
          0.3534272015094757,
          0.355142205953598,
          0.34903109073638916,
          0.35145607590675354,
          0.3529808819293976,
          0.3479502201080322,
          0.34896063804626465,
          0.35114189982414246,
          0.34738337993621826,
          0.34658050537109375,
          0.34877854585647583,
          0.3469947278499603,
          0.34476596117019653,
          0.34579381346702576,
          0.3463312089443207,
          0.3444247245788574,
          0.343178391456604,
          0.3439518213272095,
          0.3442053198814392,
          0.3427411913871765,
          0.3414134979248047,
          0.34134700894355774,
          0.3416527509689331,
          0.3413242995738983,
          0.34022215008735657,
          0.3391433358192444,
          0.3385019600391388,
          0.33825650811195374,
          0.33822983503341675,
          0.3380187451839447,
          0.33775046467781067,
          0.3373319208621979,
          0.3369966447353363,
          0.33667293190956116,
          0.33659207820892334,
          0.33682575821876526,
          0.33812934160232544,
          0.3412216901779175,
          0.3494696021080017,
          0.36153626441955566,
          0.3813161551952362,
          0.3671474754810333,
          0.344476580619812,
          0.3314858078956604,
          0.34505635499954224,
          0.36538782715797424,
          0.3572919964790344,
          0.33816710114479065,
          0.33464863896369934,
          0.34858250617980957,
          0.3525937795639038,
          0.33465370535850525,
          0.331102579832077,
          0.3413897454738617,
          0.3363681435585022,
          0.32900160551071167,
          0.3335447907447815,
          0.3372369408607483,
          0.3326266407966614,
          0.328753262758255,
          0.3322948217391968,
          0.33375999331474304,
          0.32921460270881653,
          0.3278472125530243,
          0.3307700753211975,
          0.3304179012775421,
          0.32730647921562195,
          0.32699310779571533,
          0.3291461169719696,
          0.32926294207572937,
          0.3268064260482788,
          0.3257298469543457,
          0.3265683650970459,
          0.3273434340953827,
          0.32691365480422974,
          0.3254508674144745,
          0.32451921701431274,
          0.32468509674072266,
          0.3251565992832184,
          0.3254353404045105,
          0.32509472966194153,
          0.32440367341041565,
          0.32374700903892517,
          0.32326188683509827,
          0.3228769600391388,
          0.3227396309375763,
          0.32268136739730835,
          0.32271113991737366,
          0.32305803894996643,
          0.3238038718700409,
          0.3257025480270386,
          0.33065780997276306,
          0.3432775139808655,
          0.36220982670783997,
          0.3915766477584839,
          0.3586689233779907,
          0.3289129137992859,
          0.32409554719924927,
          0.34758177399635315,
          0.36794209480285645,
          0.34264206886291504,
          0.3255000412464142,
          0.3339731693267822,
          0.33959218859672546,
          0.33315879106521606,
          0.3217040002346039,
          0.3263166844844818,
          0.3335336744785309,
          0.32387274503707886,
          0.3221690356731415,
          0.3308480679988861,
          0.3276684284210205,
          0.3207590878009796,
          0.32289379835128784,
          0.3262130916118622,
          0.3227112591266632,
          0.3199955224990845,
          0.323016494512558,
          0.32444989681243896,
          0.3213874101638794,
          0.3197123408317566,
          0.3212476968765259,
          0.32199591398239136,
          0.320655882358551,
          0.3186604380607605,
          0.3189443051815033,
          0.3199659585952759,
          0.31942862272262573,
          0.3183775544166565,
          0.3179936110973358,
          0.3181852400302887,
          0.3186474144458771,
          0.31852930784225464,
          0.317746639251709,
          0.3171254098415375,
          0.3170664608478546,
          0.31725186109542847,
          0.3175789415836334,
          0.3178935647010803,
          0.3177623152732849,
          0.3175716698169708,
          0.31734392046928406,
          0.31726357340812683,
          0.31725752353668213,
          0.31757137179374695,
          0.3180605471134186,
          0.3195810317993164,
          0.3228004574775696,
          0.33073073625564575,
          0.3420679569244385,
          0.36202922463417053,
          0.35665905475616455,
          0.3452279567718506,
          0.3211860954761505,
          0.3155837059020996,
          0.3271123766899109,
          0.3401453495025635,
          0.34253451228141785,
          0.3226051330566406,
          0.31542080640792847,
          0.3272833526134491,
          0.3357401490211487,
          0.33082109689712524,
          0.31716489791870117,
          0.31737959384918213,
          0.32610711455345154,
          0.32482215762138367,
          0.3170837163925171,
          0.3144100308418274,
          0.319171279668808,
          0.3206566274166107,
          0.31549781560897827,
          0.3140818476676941,
          0.3174770176410675,
          0.3187987804412842,
          0.31662389636039734,
          0.3136392831802368,
          0.314464271068573,
          0.3172570466995239,
          0.31713536381721497,
          0.3146006762981415,
          0.3129071295261383,
          0.313792884349823,
          0.315405011177063,
          0.3151448667049408,
          0.3135681450366974,
          0.3122784495353699,
          0.3124334514141083,
          0.3133087158203125,
          0.3138637840747833,
          0.3139576017856598,
          0.3132440149784088,
          0.3123977482318878,
          0.31167957186698914,
          0.3112282454967499,
          0.3111283481121063,
          0.31128430366516113,
          0.31159457564353943,
          0.3119998276233673,
          0.31270352005958557,
          0.3136478364467621,
          0.3160654902458191,
          0.31982728838920593,
          0.3285575211048126,
          0.33686959743499756,
          0.3507119417190552,
          0.3427148759365082,
          0.33124831318855286,
          0.3153631091117859,
          0.31013721227645874,
          0.31526699662208557,
          0.3246091306209564,
          0.33212098479270935,
          0.32354816794395447,
          0.3133533298969269,
          0.30957186222076416,
          0.31472906470298767,
          0.3232109844684601,
          0.3246564567089081,
          0.3196699619293213,
          0.3102356791496277,
          0.3110477924346924,
          0.31923678517341614,
          0.3227255046367645,
          0.319694846868515,
          0.31091436743736267,
          0.3099892735481262,
          0.31730523705482483,
          0.3208093047142029,
          0.31830254197120667,
          0.3110086917877197,
          0.3086414039134979,
          0.3126853108406067,
          0.3161125183105469,
          0.3153659403324127,
          0.3108413517475128,
          0.3084717392921448,
          0.30819520354270935,
          0.31001049280166626,
          0.3123083710670471,
          0.3110154867172241,
          0.3088756799697876,
          0.3070986866950989,
          0.3069532811641693,
          0.3088298439979553,
          0.31067103147506714,
          0.31134140491485596,
          0.31026825308799744,
          0.3080240786075592,
          0.306539922952652,
          0.30592718720436096,
          0.3059646785259247,
          0.3071080446243286,
          0.30862507224082947,
          0.30910244584083557,
          0.3097420930862427,
          0.3098852038383484,
          0.3117285966873169,
          0.3118382692337036,
          0.31267112493515015,
          0.31169044971466064,
          0.3119663894176483,
          0.3119732141494751,
          0.3131427764892578,
          0.31335344910621643,
          0.31332913041114807,
          0.3112524151802063,
          0.3106936812400818,
          0.31091079115867615,
          0.31119030714035034,
          0.3100358247756958,
          0.30919626355171204,
          0.3077794909477234,
          0.30749884247779846,
          0.307188481092453,
          0.3076491951942444,
          0.3072425425052643,
          0.3080722987651825,
          0.3077332079410553,
          0.31013965606689453,
          0.31310445070266724,
          0.31802794337272644,
          0.3192090094089508,
          0.32088395953178406,
          0.317398339509964,
          0.3141438961029053,
          0.3086710572242737,
          0.30446237325668335,
          0.30196380615234375,
          0.3025566339492798,
          0.3035520911216736,
          0.3062705099582672,
          0.3094976842403412,
          0.3113763928413391,
          0.31400230526924133,
          0.31451770663261414,
          0.31493476033210754,
          0.3103790581226349,
          0.306227445602417,
          0.3027244210243225,
          0.30079159140586853,
          0.30239760875701904,
          0.3021681308746338,
          0.304298460483551,
          0.3062284588813782,
          0.3089679479598999,
          0.3118003308773041,
          0.31548455357551575,
          0.31429314613342285,
          0.31281012296676636,
          0.30952414870262146,
          0.3066914975643158,
          0.3034619987010956,
          0.30091720819473267,
          0.2995041310787201,
          0.3010011315345764,
          0.3012624680995941,
          0.3028930127620697,
          0.305161714553833,
          0.30669155716896057,
          0.3088770806789398,
          0.31071737408638,
          0.31354963779449463,
          0.313128799200058,
          0.31229811906814575,
          0.3084745705127716,
          0.30542927980422974,
          0.30190610885620117,
          0.2997368574142456,
          0.2986442446708679,
          0.2996704578399658,
          0.30047518014907837,
          0.3014582395553589,
          0.3034345209598541,
          0.3050816059112549,
          0.3076596260070801,
          0.3110940158367157,
          0.3152773976325989,
          0.31514281034469604,
          0.31390380859375,
          0.30841684341430664,
          0.3037816286087036,
          0.29999610781669617,
          0.29877015948295593,
          0.2997598946094513,
          0.3024986982345581,
          0.30665674805641174,
          0.31111031770706177,
          0.3142586946487427,
          0.31286105513572693,
          0.3097588121891022,
          0.30491402745246887,
          0.300648957490921,
          0.2974347174167633,
          0.29690051078796387,
          0.2987652122974396,
          0.30055856704711914,
          0.3018512427806854
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=list(range(epochs)), y=list(map(lambda x: x.item(), final_losses)))\n",
    "fig.update_layout(xaxis_title='Epoch', yaxis_title='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8246753246753247"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_test):\n",
    "        y_pred=model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y_test,predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.623051  [   16/  537]\n",
      "loss: 0.710709  [   96/  537]\n",
      "loss: 0.689016  [  176/  537]\n",
      "loss: 0.609918  [  256/  537]\n",
      "loss: 0.759521  [  336/  537]\n",
      "loss: 0.595187  [  416/  537]\n",
      "loss: 0.662290  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652138 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.587295  [   16/  537]\n",
      "loss: 0.740669  [   96/  537]\n",
      "loss: 0.741760  [  176/  537]\n",
      "loss: 0.663411  [  256/  537]\n",
      "loss: 0.784175  [  336/  537]\n",
      "loss: 0.586118  [  416/  537]\n",
      "loss: 0.782245  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655491 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.701184  [   16/  537]\n",
      "loss: 0.700722  [   96/  537]\n",
      "loss: 0.623587  [  176/  537]\n",
      "loss: 0.583861  [  256/  537]\n",
      "loss: 0.583432  [  336/  537]\n",
      "loss: 0.542940  [  416/  537]\n",
      "loss: 0.744505  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655557 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.701636  [   16/  537]\n",
      "loss: 0.662767  [   96/  537]\n",
      "loss: 0.624709  [  176/  537]\n",
      "loss: 0.625948  [  256/  537]\n",
      "loss: 0.695808  [  336/  537]\n",
      "loss: 0.699043  [  416/  537]\n",
      "loss: 0.550205  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649092 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.623217  [   16/  537]\n",
      "loss: 0.663339  [   96/  537]\n",
      "loss: 0.662855  [  176/  537]\n",
      "loss: 0.586906  [  256/  537]\n",
      "loss: 0.704566  [  336/  537]\n",
      "loss: 0.787897  [  416/  537]\n",
      "loss: 0.663660  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649057 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.542739  [   16/  537]\n",
      "loss: 0.506869  [   96/  537]\n",
      "loss: 0.702931  [  176/  537]\n",
      "loss: 0.584751  [  256/  537]\n",
      "loss: 0.702733  [  336/  537]\n",
      "loss: 0.624630  [  416/  537]\n",
      "loss: 0.623973  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652500 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.663616  [   16/  537]\n",
      "loss: 0.623809  [   96/  537]\n",
      "loss: 0.623852  [  176/  537]\n",
      "loss: 0.663869  [  256/  537]\n",
      "loss: 0.584412  [  336/  537]\n",
      "loss: 0.624088  [  416/  537]\n",
      "loss: 0.663294  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652496 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.583149  [   16/  537]\n",
      "loss: 0.492570  [   96/  537]\n",
      "loss: 0.621650  [  176/  537]\n",
      "loss: 0.621903  [  256/  537]\n",
      "loss: 0.843736  [  336/  537]\n",
      "loss: 0.707550  [  416/  537]\n",
      "loss: 0.704320  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648913 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.701256  [   16/  537]\n",
      "loss: 0.697755  [   96/  537]\n",
      "loss: 0.627368  [  176/  537]\n",
      "loss: 0.662023  [  256/  537]\n",
      "loss: 0.736697  [  336/  537]\n",
      "loss: 0.587807  [  416/  537]\n",
      "loss: 0.662822  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649215 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.664158  [   16/  537]\n",
      "loss: 0.583404  [   96/  537]\n",
      "loss: 0.784185  [  176/  537]\n",
      "loss: 0.663391  [  256/  537]\n",
      "loss: 0.705040  [  336/  537]\n",
      "loss: 0.581847  [  416/  537]\n",
      "loss: 0.705255  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645607 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.744446  [   16/  537]\n",
      "loss: 0.855315  [   96/  537]\n",
      "loss: 0.662649  [  176/  537]\n",
      "loss: 0.584453  [  256/  537]\n",
      "loss: 0.543507  [  336/  537]\n",
      "loss: 0.581725  [  416/  537]\n",
      "loss: 0.707024  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.642147 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.582982  [   16/  537]\n",
      "loss: 0.579432  [   96/  537]\n",
      "loss: 0.708233  [  176/  537]\n",
      "loss: 0.581113  [  256/  537]\n",
      "loss: 0.749230  [  336/  537]\n",
      "loss: 0.547892  [  416/  537]\n",
      "loss: 0.584674  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655548 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.817583  [   16/  537]\n",
      "loss: 0.700308  [   96/  537]\n",
      "loss: 0.510052  [  176/  537]\n",
      "loss: 0.623616  [  256/  537]\n",
      "loss: 0.663057  [  336/  537]\n",
      "loss: 0.623669  [  416/  537]\n",
      "loss: 0.624507  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645591 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.545980  [   16/  537]\n",
      "loss: 0.623363  [   96/  537]\n",
      "loss: 0.704586  [  176/  537]\n",
      "loss: 0.623058  [  256/  537]\n",
      "loss: 0.780843  [  336/  537]\n",
      "loss: 0.663625  [  416/  537]\n",
      "loss: 0.511386  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652221 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.585753  [   16/  537]\n",
      "loss: 0.623250  [   96/  537]\n",
      "loss: 0.705409  [  176/  537]\n",
      "loss: 0.664400  [  256/  537]\n",
      "loss: 0.623103  [  336/  537]\n",
      "loss: 0.705354  [  416/  537]\n",
      "loss: 0.663345  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.642412 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.624726  [   16/  537]\n",
      "loss: 0.543474  [   96/  537]\n",
      "loss: 0.702824  [  176/  537]\n",
      "loss: 0.547350  [  256/  537]\n",
      "loss: 0.704136  [  336/  537]\n",
      "loss: 0.624276  [  416/  537]\n",
      "loss: 0.779776  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645598 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.585322  [   16/  537]\n",
      "loss: 0.623988  [   96/  537]\n",
      "loss: 0.701771  [  176/  537]\n",
      "loss: 0.585359  [  256/  537]\n",
      "loss: 0.505563  [  336/  537]\n",
      "loss: 0.706351  [  416/  537]\n",
      "loss: 0.623694  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645620 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.586002  [   16/  537]\n",
      "loss: 0.662940  [   96/  537]\n",
      "loss: 0.700810  [  176/  537]\n",
      "loss: 0.549060  [  256/  537]\n",
      "loss: 0.623914  [  336/  537]\n",
      "loss: 0.623962  [  416/  537]\n",
      "loss: 0.663693  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652238 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.779020  [   16/  537]\n",
      "loss: 0.553776  [   96/  537]\n",
      "loss: 0.624942  [  176/  537]\n",
      "loss: 0.662768  [  256/  537]\n",
      "loss: 0.701091  [  336/  537]\n",
      "loss: 0.662930  [  416/  537]\n",
      "loss: 0.704602  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655696 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.584551  [   16/  537]\n",
      "loss: 0.741634  [   96/  537]\n",
      "loss: 0.701392  [  176/  537]\n",
      "loss: 0.624304  [  256/  537]\n",
      "loss: 0.662873  [  336/  537]\n",
      "loss: 0.623316  [  416/  537]\n",
      "loss: 0.583278  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.659188 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.583995  [   16/  537]\n",
      "loss: 0.623366  [   96/  537]\n",
      "loss: 0.539682  [  176/  537]\n",
      "loss: 0.622464  [  256/  537]\n",
      "loss: 0.705227  [  336/  537]\n",
      "loss: 0.623198  [  416/  537]\n",
      "loss: 0.702921  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655587 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.585292  [   16/  537]\n",
      "loss: 0.778081  [   96/  537]\n",
      "loss: 0.587680  [  176/  537]\n",
      "loss: 0.624160  [  256/  537]\n",
      "loss: 0.738030  [  336/  537]\n",
      "loss: 0.625180  [  416/  537]\n",
      "loss: 0.472404  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655988 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.663678  [   16/  537]\n",
      "loss: 0.744008  [   96/  537]\n",
      "loss: 0.581822  [  176/  537]\n",
      "loss: 0.622616  [  256/  537]\n",
      "loss: 0.664943  [  336/  537]\n",
      "loss: 0.665777  [  416/  537]\n",
      "loss: 0.743488  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652369 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.623725  [   16/  537]\n",
      "loss: 0.704178  [   96/  537]\n",
      "loss: 0.546230  [  176/  537]\n",
      "loss: 0.623380  [  256/  537]\n",
      "loss: 0.703603  [  336/  537]\n",
      "loss: 0.746000  [  416/  537]\n",
      "loss: 0.546279  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648935 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.624049  [   16/  537]\n",
      "loss: 0.705096  [   96/  537]\n",
      "loss: 0.541223  [  176/  537]\n",
      "loss: 0.506552  [  256/  537]\n",
      "loss: 0.584561  [  336/  537]\n",
      "loss: 0.624292  [  416/  537]\n",
      "loss: 0.623734  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655594 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.624119  [   16/  537]\n",
      "loss: 0.624147  [   96/  537]\n",
      "loss: 0.781551  [  176/  537]\n",
      "loss: 0.662818  [  256/  537]\n",
      "loss: 0.662598  [  336/  537]\n",
      "loss: 0.624283  [  416/  537]\n",
      "loss: 0.783255  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645585 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.702871  [   16/  537]\n",
      "loss: 0.704255  [   96/  537]\n",
      "loss: 0.623148  [  176/  537]\n",
      "loss: 0.704241  [  256/  537]\n",
      "loss: 0.582959  [  336/  537]\n",
      "loss: 0.623083  [  416/  537]\n",
      "loss: 0.703267  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.642240 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.624000  [   16/  537]\n",
      "loss: 0.583436  [   96/  537]\n",
      "loss: 0.705967  [  176/  537]\n",
      "loss: 0.624609  [  256/  537]\n",
      "loss: 0.545160  [  336/  537]\n",
      "loss: 0.623722  [  416/  537]\n",
      "loss: 0.663267  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652342 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.663228  [   16/  537]\n",
      "loss: 0.623869  [   96/  537]\n",
      "loss: 0.623234  [  176/  537]\n",
      "loss: 0.663393  [  256/  537]\n",
      "loss: 0.624039  [  336/  537]\n",
      "loss: 0.584138  [  416/  537]\n",
      "loss: 0.662873  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655890 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.543381  [   16/  537]\n",
      "loss: 0.623536  [   96/  537]\n",
      "loss: 0.623277  [  176/  537]\n",
      "loss: 0.663471  [  256/  537]\n",
      "loss: 0.623081  [  336/  537]\n",
      "loss: 0.786836  [  416/  537]\n",
      "loss: 0.785163  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.662505 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.663257  [   16/  537]\n",
      "loss: 0.473807  [   96/  537]\n",
      "loss: 0.662466  [  176/  537]\n",
      "loss: 0.586298  [  256/  537]\n",
      "loss: 0.663597  [  336/  537]\n",
      "loss: 0.741987  [  416/  537]\n",
      "loss: 0.624025  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645591 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.623512  [   16/  537]\n",
      "loss: 0.742383  [   96/  537]\n",
      "loss: 0.585153  [  176/  537]\n",
      "loss: 0.582825  [  256/  537]\n",
      "loss: 0.747293  [  336/  537]\n",
      "loss: 0.741979  [  416/  537]\n",
      "loss: 0.586961  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645606 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.542577  [   16/  537]\n",
      "loss: 0.582288  [   96/  537]\n",
      "loss: 0.705777  [  176/  537]\n",
      "loss: 0.622475  [  256/  537]\n",
      "loss: 0.623199  [  336/  537]\n",
      "loss: 0.581378  [  416/  537]\n",
      "loss: 0.746443  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652337 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.584447  [   16/  537]\n",
      "loss: 0.744254  [   96/  537]\n",
      "loss: 0.585199  [  176/  537]\n",
      "loss: 0.705429  [  256/  537]\n",
      "loss: 0.705183  [  336/  537]\n",
      "loss: 0.748866  [  416/  537]\n",
      "loss: 0.742410  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649012 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.623535  [   16/  537]\n",
      "loss: 0.624062  [   96/  537]\n",
      "loss: 0.662491  [  176/  537]\n",
      "loss: 0.776420  [  256/  537]\n",
      "loss: 0.663391  [  336/  537]\n",
      "loss: 0.663600  [  416/  537]\n",
      "loss: 0.704953  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645629 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.786118  [   16/  537]\n",
      "loss: 0.465543  [   96/  537]\n",
      "loss: 0.745536  [  176/  537]\n",
      "loss: 0.583758  [  256/  537]\n",
      "loss: 0.704324  [  336/  537]\n",
      "loss: 0.623265  [  416/  537]\n",
      "loss: 0.664069  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645609 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.623280  [   16/  537]\n",
      "loss: 0.585797  [   96/  537]\n",
      "loss: 0.782385  [  176/  537]\n",
      "loss: 0.548051  [  256/  537]\n",
      "loss: 0.742657  [  336/  537]\n",
      "loss: 0.623723  [  416/  537]\n",
      "loss: 0.585188  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649103 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.663805  [   16/  537]\n",
      "loss: 0.706471  [   96/  537]\n",
      "loss: 0.744731  [  176/  537]\n",
      "loss: 0.543213  [  256/  537]\n",
      "loss: 0.583077  [  336/  537]\n",
      "loss: 0.705437  [  416/  537]\n",
      "loss: 0.746998  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652335 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.584461  [   16/  537]\n",
      "loss: 0.701218  [   96/  537]\n",
      "loss: 0.623862  [  176/  537]\n",
      "loss: 0.664500  [  256/  537]\n",
      "loss: 0.623261  [  336/  537]\n",
      "loss: 0.745660  [  416/  537]\n",
      "loss: 0.741655  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645585 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.702774  [   16/  537]\n",
      "loss: 0.625084  [   96/  537]\n",
      "loss: 0.551721  [  176/  537]\n",
      "loss: 0.624924  [  256/  537]\n",
      "loss: 0.624478  [  336/  537]\n",
      "loss: 0.663033  [  416/  537]\n",
      "loss: 0.543834  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649008 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.663446  [   16/  537]\n",
      "loss: 0.701441  [   96/  537]\n",
      "loss: 0.701931  [  176/  537]\n",
      "loss: 0.701972  [  256/  537]\n",
      "loss: 0.743129  [  336/  537]\n",
      "loss: 0.584776  [  416/  537]\n",
      "loss: 0.663391  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.656153 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.541281  [   16/  537]\n",
      "loss: 0.748425  [   96/  537]\n",
      "loss: 0.664378  [  176/  537]\n",
      "loss: 0.581822  [  256/  537]\n",
      "loss: 0.744070  [  336/  537]\n",
      "loss: 0.544692  [  416/  537]\n",
      "loss: 0.743634  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645657 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.540794  [   16/  537]\n",
      "loss: 0.745691  [   96/  537]\n",
      "loss: 0.664193  [  176/  537]\n",
      "loss: 0.664594  [  256/  537]\n",
      "loss: 0.544722  [  336/  537]\n",
      "loss: 0.702165  [  416/  537]\n",
      "loss: 0.623183  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645600 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.623376  [   16/  537]\n",
      "loss: 0.742144  [   96/  537]\n",
      "loss: 0.624546  [  176/  537]\n",
      "loss: 0.624534  [  256/  537]\n",
      "loss: 0.586822  [  336/  537]\n",
      "loss: 0.623715  [  416/  537]\n",
      "loss: 0.703645  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.656179 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.623019  [   16/  537]\n",
      "loss: 0.623111  [   96/  537]\n",
      "loss: 0.545304  [  176/  537]\n",
      "loss: 0.623392  [  256/  537]\n",
      "loss: 0.624039  [  336/  537]\n",
      "loss: 0.663475  [  416/  537]\n",
      "loss: 0.541376  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645609 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.704140  [   16/  537]\n",
      "loss: 0.584051  [   96/  537]\n",
      "loss: 0.538093  [  176/  537]\n",
      "loss: 0.709626  [  256/  537]\n",
      "loss: 0.666397  [  336/  537]\n",
      "loss: 0.753347  [  416/  537]\n",
      "loss: 0.705961  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648909 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.548930  [   16/  537]\n",
      "loss: 0.663018  [   96/  537]\n",
      "loss: 0.543666  [  176/  537]\n",
      "loss: 0.663278  [  256/  537]\n",
      "loss: 0.662954  [  336/  537]\n",
      "loss: 0.547356  [  416/  537]\n",
      "loss: 0.663115  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652165 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.700661  [   16/  537]\n",
      "loss: 0.625005  [   96/  537]\n",
      "loss: 0.699735  [  176/  537]\n",
      "loss: 0.510538  [  256/  537]\n",
      "loss: 0.585130  [  336/  537]\n",
      "loss: 0.584193  [  416/  537]\n",
      "loss: 0.740547  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655481 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.662781  [   16/  537]\n",
      "loss: 0.624066  [   96/  537]\n",
      "loss: 0.741703  [  176/  537]\n",
      "loss: 0.663016  [  256/  537]\n",
      "loss: 0.663309  [  336/  537]\n",
      "loss: 0.585852  [  416/  537]\n",
      "loss: 0.588539  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655678 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.584665  [   16/  537]\n",
      "loss: 0.624329  [   96/  537]\n",
      "loss: 0.781595  [  176/  537]\n",
      "loss: 0.740452  [  256/  537]\n",
      "loss: 0.548211  [  336/  537]\n",
      "loss: 0.821057  [  416/  537]\n",
      "loss: 0.742142  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652355 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.663263  [   16/  537]\n",
      "loss: 0.546664  [   96/  537]\n",
      "loss: 0.582584  [  176/  537]\n",
      "loss: 0.622814  [  256/  537]\n",
      "loss: 0.537594  [  336/  537]\n",
      "loss: 0.708098  [  416/  537]\n",
      "loss: 0.829228  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655653 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.623970  [   16/  537]\n",
      "loss: 0.582153  [   96/  537]\n",
      "loss: 0.703159  [  176/  537]\n",
      "loss: 0.545849  [  256/  537]\n",
      "loss: 0.819065  [  336/  537]\n",
      "loss: 0.624773  [  416/  537]\n",
      "loss: 0.624557  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648969 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.584286  [   16/  537]\n",
      "loss: 0.584485  [   96/  537]\n",
      "loss: 0.702888  [  176/  537]\n",
      "loss: 0.662862  [  256/  537]\n",
      "loss: 0.585707  [  336/  537]\n",
      "loss: 0.663782  [  416/  537]\n",
      "loss: 0.663238  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652398 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.742841  [   16/  537]\n",
      "loss: 0.623336  [   96/  537]\n",
      "loss: 0.704199  [  176/  537]\n",
      "loss: 0.623005  [  256/  537]\n",
      "loss: 0.740045  [  336/  537]\n",
      "loss: 0.703362  [  416/  537]\n",
      "loss: 0.623577  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652404 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.583849  [   16/  537]\n",
      "loss: 0.541048  [   96/  537]\n",
      "loss: 0.664389  [  176/  537]\n",
      "loss: 0.753079  [  256/  537]\n",
      "loss: 0.582884  [  336/  537]\n",
      "loss: 0.743282  [  416/  537]\n",
      "loss: 0.825681  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652385 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.623676  [   16/  537]\n",
      "loss: 0.540179  [   96/  537]\n",
      "loss: 0.705611  [  176/  537]\n",
      "loss: 0.664812  [  256/  537]\n",
      "loss: 0.748830  [  336/  537]\n",
      "loss: 0.742768  [  416/  537]\n",
      "loss: 0.741974  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655530 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.740037  [   16/  537]\n",
      "loss: 0.739438  [   96/  537]\n",
      "loss: 0.662378  [  176/  537]\n",
      "loss: 0.662532  [  256/  537]\n",
      "loss: 0.586566  [  336/  537]\n",
      "loss: 0.744139  [  416/  537]\n",
      "loss: 0.703258  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648993 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.623632  [   16/  537]\n",
      "loss: 0.624852  [   96/  537]\n",
      "loss: 0.663355  [  176/  537]\n",
      "loss: 0.508276  [  256/  537]\n",
      "loss: 0.663141  [  336/  537]\n",
      "loss: 0.583786  [  416/  537]\n",
      "loss: 0.787977  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.659072 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.702527  [   16/  537]\n",
      "loss: 0.778806  [   96/  537]\n",
      "loss: 0.624070  [  176/  537]\n",
      "loss: 0.624304  [  256/  537]\n",
      "loss: 0.703857  [  336/  537]\n",
      "loss: 0.622952  [  416/  537]\n",
      "loss: 0.663764  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652267 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.507332  [   16/  537]\n",
      "loss: 0.700485  [   96/  537]\n",
      "loss: 0.663161  [  176/  537]\n",
      "loss: 0.624143  [  256/  537]\n",
      "loss: 0.662599  [  336/  537]\n",
      "loss: 0.740799  [  416/  537]\n",
      "loss: 0.703311  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648996 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.623614  [   16/  537]\n",
      "loss: 0.704512  [   96/  537]\n",
      "loss: 0.663427  [  176/  537]\n",
      "loss: 0.624370  [  256/  537]\n",
      "loss: 0.662482  [  336/  537]\n",
      "loss: 0.587360  [  416/  537]\n",
      "loss: 0.585122  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649053 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.583087  [   16/  537]\n",
      "loss: 0.664069  [   96/  537]\n",
      "loss: 0.663769  [  176/  537]\n",
      "loss: 0.663520  [  256/  537]\n",
      "loss: 0.623376  [  336/  537]\n",
      "loss: 0.622514  [  416/  537]\n",
      "loss: 0.583405  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648926 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.701782  [   16/  537]\n",
      "loss: 0.623112  [   96/  537]\n",
      "loss: 0.542095  [  176/  537]\n",
      "loss: 0.622986  [  256/  537]\n",
      "loss: 0.623925  [  336/  537]\n",
      "loss: 0.583817  [  416/  537]\n",
      "loss: 0.702031  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645605 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.740269  [   16/  537]\n",
      "loss: 0.699811  [   96/  537]\n",
      "loss: 0.624993  [  176/  537]\n",
      "loss: 0.701075  [  256/  537]\n",
      "loss: 0.512950  [  336/  537]\n",
      "loss: 0.624370  [  416/  537]\n",
      "loss: 0.544676  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648950 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.820088  [   16/  537]\n",
      "loss: 0.625182  [   96/  537]\n",
      "loss: 0.663460  [  176/  537]\n",
      "loss: 0.623423  [  256/  537]\n",
      "loss: 0.623014  [  336/  537]\n",
      "loss: 0.663531  [  416/  537]\n",
      "loss: 0.663165  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.642252 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.663038  [   16/  537]\n",
      "loss: 0.623183  [   96/  537]\n",
      "loss: 0.705118  [  176/  537]\n",
      "loss: 0.748403  [  256/  537]\n",
      "loss: 0.543511  [  336/  537]\n",
      "loss: 0.584291  [  416/  537]\n",
      "loss: 0.702585  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652238 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.585523  [   16/  537]\n",
      "loss: 0.543507  [   96/  537]\n",
      "loss: 0.545494  [  176/  537]\n",
      "loss: 0.543155  [  256/  537]\n",
      "loss: 0.819166  [  336/  537]\n",
      "loss: 0.547674  [  416/  537]\n",
      "loss: 0.701983  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652240 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.624215  [   16/  537]\n",
      "loss: 0.701604  [   96/  537]\n",
      "loss: 0.624826  [  176/  537]\n",
      "loss: 0.662193  [  256/  537]\n",
      "loss: 0.698981  [  336/  537]\n",
      "loss: 0.589585  [  416/  537]\n",
      "loss: 0.624378  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649035 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.663555  [   16/  537]\n",
      "loss: 0.663315  [   96/  537]\n",
      "loss: 0.623075  [  176/  537]\n",
      "loss: 0.663970  [  256/  537]\n",
      "loss: 0.623636  [  336/  537]\n",
      "loss: 0.623539  [  416/  537]\n",
      "loss: 0.582240  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648939 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.663076  [   16/  537]\n",
      "loss: 0.547768  [   96/  537]\n",
      "loss: 0.624879  [  176/  537]\n",
      "loss: 0.739729  [  256/  537]\n",
      "loss: 0.700853  [  336/  537]\n",
      "loss: 0.701405  [  416/  537]\n",
      "loss: 0.549796  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652561 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.663753  [   16/  537]\n",
      "loss: 0.582092  [   96/  537]\n",
      "loss: 0.663362  [  176/  537]\n",
      "loss: 0.747399  [  256/  537]\n",
      "loss: 0.623489  [  336/  537]\n",
      "loss: 0.706292  [  416/  537]\n",
      "loss: 0.747352  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655870 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.623510  [   16/  537]\n",
      "loss: 0.662990  [   96/  537]\n",
      "loss: 0.701846  [  176/  537]\n",
      "loss: 0.584803  [  256/  537]\n",
      "loss: 0.535838  [  336/  537]\n",
      "loss: 0.536167  [  416/  537]\n",
      "loss: 0.622877  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652242 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.624210  [   16/  537]\n",
      "loss: 0.702943  [   96/  537]\n",
      "loss: 0.624446  [  176/  537]\n",
      "loss: 0.550218  [  256/  537]\n",
      "loss: 0.585443  [  336/  537]\n",
      "loss: 0.784731  [  416/  537]\n",
      "loss: 0.663031  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652369 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.782026  [   16/  537]\n",
      "loss: 0.542214  [   96/  537]\n",
      "loss: 0.701581  [  176/  537]\n",
      "loss: 0.623516  [  256/  537]\n",
      "loss: 0.623119  [  336/  537]\n",
      "loss: 0.622508  [  416/  537]\n",
      "loss: 0.746424  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652299 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.741392  [   16/  537]\n",
      "loss: 0.699178  [   96/  537]\n",
      "loss: 0.776737  [  176/  537]\n",
      "loss: 0.625640  [  256/  537]\n",
      "loss: 0.663576  [  336/  537]\n",
      "loss: 0.662732  [  416/  537]\n",
      "loss: 0.623955  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652520 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.542647  [   16/  537]\n",
      "loss: 0.622819  [   96/  537]\n",
      "loss: 0.794436  [  176/  537]\n",
      "loss: 0.705185  [  256/  537]\n",
      "loss: 0.622746  [  336/  537]\n",
      "loss: 0.704957  [  416/  537]\n",
      "loss: 0.663404  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645585 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.623807  [   16/  537]\n",
      "loss: 0.504797  [   96/  537]\n",
      "loss: 0.624464  [  176/  537]\n",
      "loss: 0.587140  [  256/  537]\n",
      "loss: 0.587507  [  336/  537]\n",
      "loss: 0.587383  [  416/  537]\n",
      "loss: 0.740092  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649081 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.623256  [   16/  537]\n",
      "loss: 0.623198  [   96/  537]\n",
      "loss: 0.663089  [  176/  537]\n",
      "loss: 0.701190  [  256/  537]\n",
      "loss: 0.623289  [  336/  537]\n",
      "loss: 0.623062  [  416/  537]\n",
      "loss: 0.663823  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648985 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.584012  [   16/  537]\n",
      "loss: 0.784158  [   96/  537]\n",
      "loss: 0.544972  [  176/  537]\n",
      "loss: 0.779172  [  256/  537]\n",
      "loss: 0.739803  [  336/  537]\n",
      "loss: 0.663394  [  416/  537]\n",
      "loss: 0.583062  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652427 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.583670  [   16/  537]\n",
      "loss: 0.623123  [   96/  537]\n",
      "loss: 0.741985  [  176/  537]\n",
      "loss: 0.663220  [  256/  537]\n",
      "loss: 0.868332  [  336/  537]\n",
      "loss: 0.623120  [  416/  537]\n",
      "loss: 0.703854  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655838 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.663430  [   16/  537]\n",
      "loss: 0.623445  [   96/  537]\n",
      "loss: 0.828906  [  176/  537]\n",
      "loss: 0.580212  [  256/  537]\n",
      "loss: 0.665733  [  336/  537]\n",
      "loss: 0.536107  [  416/  537]\n",
      "loss: 0.926675  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645609 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.662883  [   16/  537]\n",
      "loss: 0.701945  [   96/  537]\n",
      "loss: 0.663140  [  176/  537]\n",
      "loss: 0.625044  [  256/  537]\n",
      "loss: 0.549976  [  336/  537]\n",
      "loss: 0.512226  [  416/  537]\n",
      "loss: 0.662522  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645587 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.545490  [   16/  537]\n",
      "loss: 0.623356  [   96/  537]\n",
      "loss: 0.623740  [  176/  537]\n",
      "loss: 0.584843  [  256/  537]\n",
      "loss: 0.623365  [  336/  537]\n",
      "loss: 0.623759  [  416/  537]\n",
      "loss: 0.745079  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655619 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.741034  [   16/  537]\n",
      "loss: 0.775848  [   96/  537]\n",
      "loss: 0.699659  [  176/  537]\n",
      "loss: 0.554534  [  256/  537]\n",
      "loss: 0.625528  [  336/  537]\n",
      "loss: 0.700434  [  416/  537]\n",
      "loss: 0.663134  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649079 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.663722  [   16/  537]\n",
      "loss: 0.707357  [   96/  537]\n",
      "loss: 0.623346  [  176/  537]\n",
      "loss: 0.663412  [  256/  537]\n",
      "loss: 0.623146  [  336/  537]\n",
      "loss: 0.546943  [  416/  537]\n",
      "loss: 0.546175  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.655932 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.703774  [   16/  537]\n",
      "loss: 0.623456  [   96/  537]\n",
      "loss: 0.622615  [  176/  537]\n",
      "loss: 0.704483  [  256/  537]\n",
      "loss: 0.663752  [  336/  537]\n",
      "loss: 0.703514  [  416/  537]\n",
      "loss: 0.703414  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652376 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.544479  [   16/  537]\n",
      "loss: 0.583753  [   96/  537]\n",
      "loss: 0.583449  [  176/  537]\n",
      "loss: 0.703856  [  256/  537]\n",
      "loss: 0.580833  [  336/  537]\n",
      "loss: 0.584174  [  416/  537]\n",
      "loss: 0.584841  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648966 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.742145  [   16/  537]\n",
      "loss: 0.586544  [   96/  537]\n",
      "loss: 0.624601  [  176/  537]\n",
      "loss: 0.507670  [  256/  537]\n",
      "loss: 0.663998  [  336/  537]\n",
      "loss: 0.749690  [  416/  537]\n",
      "loss: 0.623904  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652416 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.663417  [   16/  537]\n",
      "loss: 0.704046  [   96/  537]\n",
      "loss: 0.704804  [  176/  537]\n",
      "loss: 0.539378  [  256/  537]\n",
      "loss: 0.748663  [  336/  537]\n",
      "loss: 0.743228  [  416/  537]\n",
      "loss: 0.585782  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648951 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.623902  [   16/  537]\n",
      "loss: 0.701443  [   96/  537]\n",
      "loss: 0.701019  [  176/  537]\n",
      "loss: 0.701146  [  256/  537]\n",
      "loss: 0.589672  [  336/  537]\n",
      "loss: 0.553685  [  416/  537]\n",
      "loss: 0.550458  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.652706 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.664060  [   16/  537]\n",
      "loss: 0.536879  [   96/  537]\n",
      "loss: 0.843958  [  176/  537]\n",
      "loss: 0.578653  [  256/  537]\n",
      "loss: 0.708151  [  336/  537]\n",
      "loss: 0.706664  [  416/  537]\n",
      "loss: 0.826229  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645585 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.742667  [   16/  537]\n",
      "loss: 0.703776  [   96/  537]\n",
      "loss: 0.701948  [  176/  537]\n",
      "loss: 0.548571  [  256/  537]\n",
      "loss: 0.623155  [  336/  537]\n",
      "loss: 0.785247  [  416/  537]\n",
      "loss: 0.584240  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648996 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.742931  [   16/  537]\n",
      "loss: 0.743513  [   96/  537]\n",
      "loss: 0.703387  [  176/  537]\n",
      "loss: 0.622998  [  256/  537]\n",
      "loss: 0.499593  [  336/  537]\n",
      "loss: 0.663484  [  416/  537]\n",
      "loss: 0.584190  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648935 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.546079  [   16/  537]\n",
      "loss: 0.499868  [   96/  537]\n",
      "loss: 0.622440  [  176/  537]\n",
      "loss: 0.583064  [  256/  537]\n",
      "loss: 0.663284  [  336/  537]\n",
      "loss: 0.743715  [  416/  537]\n",
      "loss: 0.700525  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.642260 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.701918  [   16/  537]\n",
      "loss: 0.624096  [   96/  537]\n",
      "loss: 0.701577  [  176/  537]\n",
      "loss: 0.699180  [  256/  537]\n",
      "loss: 0.624710  [  336/  537]\n",
      "loss: 0.624304  [  416/  537]\n",
      "loss: 0.544774  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649019 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.583510  [   16/  537]\n",
      "loss: 0.622962  [   96/  537]\n",
      "loss: 0.544998  [  176/  537]\n",
      "loss: 0.623095  [  256/  537]\n",
      "loss: 0.664282  [  336/  537]\n",
      "loss: 0.508198  [  416/  537]\n",
      "loss: 0.663387  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.642201 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.623780  [   16/  537]\n",
      "loss: 0.586346  [   96/  537]\n",
      "loss: 0.663336  [  176/  537]\n",
      "loss: 0.662919  [  256/  537]\n",
      "loss: 0.624598  [  336/  537]\n",
      "loss: 0.585896  [  416/  537]\n",
      "loss: 0.663361  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.648991 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.623643  [   16/  537]\n",
      "loss: 0.663051  [   96/  537]\n",
      "loss: 0.663464  [  176/  537]\n",
      "loss: 0.703222  [  256/  537]\n",
      "loss: 0.583498  [  336/  537]\n",
      "loss: 0.663753  [  416/  537]\n",
      "loss: 0.748524  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.645585 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.623776  [   16/  537]\n",
      "loss: 0.662934  [   96/  537]\n",
      "loss: 0.702090  [  176/  537]\n",
      "loss: 0.662679  [  256/  537]\n",
      "loss: 0.701256  [  336/  537]\n",
      "loss: 0.623762  [  416/  537]\n",
      "loss: 0.703719  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.662770 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.703611  [   16/  537]\n",
      "loss: 0.702001  [   96/  537]\n",
      "loss: 0.664203  [  176/  537]\n",
      "loss: 0.705708  [  256/  537]\n",
      "loss: 0.663111  [  336/  537]\n",
      "loss: 0.701758  [  416/  537]\n",
      "loss: 0.545641  [  496/  537]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.649004 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# From pytorch tutorial - requires dataloader\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(0) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Healthy\", Actual: \"Healthy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/49/ymyjgyx173l0zz0x9x2h95jh0000gn/T/ipykernel_72350/2717208966.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = torch.tensor(X_test[0]).to(device), torch.tensor(y_test[0]).to(device)\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    'Healthy', \n",
    "    'Diabetic'\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = torch.tensor(X_test[0]).to(device), torch.tensor(y_test[0]).to(device)\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[int(pred[0].argmax(0))], classes[int(y)]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
