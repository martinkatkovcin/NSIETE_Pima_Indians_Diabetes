{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "Using mps device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(torch.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and train-test split\n",
    "\n",
    "First, we load cleaned data and created two datasets - labels (y) and predictors (X) - which were further split into train (80%) and test (20%) sets. To ensure the split will always be the same, we also set the random state seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.314928</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.171779</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.104294</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.185096</td>\n",
       "      <td>0.509202</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.367742</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.445652</td>\n",
       "      <td>0.199519</td>\n",
       "      <td>0.300613</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.503226</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.380368</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.496774</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.117788</td>\n",
       "      <td>0.163599</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.529032</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.243354</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.316129</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.249489</td>\n",
       "      <td>0.101196</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0       0.352941  0.670968       0.489796       0.304348  0.186899  0.314928   \n",
       "1       0.058824  0.264516       0.428571       0.239130  0.106370  0.171779   \n",
       "2       0.470588  0.896774       0.408163       0.271739  0.186899  0.104294   \n",
       "3       0.058824  0.290323       0.428571       0.173913  0.096154  0.202454   \n",
       "4       0.000000  0.600000       0.163265       0.304348  0.185096  0.509202   \n",
       "..           ...       ...            ...            ...       ...       ...   \n",
       "763     0.588235  0.367742       0.530612       0.445652  0.199519  0.300613   \n",
       "764     0.117647  0.503226       0.469388       0.217391  0.106370  0.380368   \n",
       "765     0.294118  0.496774       0.489796       0.173913  0.117788  0.163599   \n",
       "766     0.058824  0.529032       0.367347       0.271739  0.186899  0.243354   \n",
       "767     0.058824  0.316129       0.469388       0.260870  0.106370  0.249489   \n",
       "\n",
       "     DiabetesPedigreeFunction       Age  Outcome  \n",
       "0                    0.234415  0.483333        1  \n",
       "1                    0.116567  0.166667        0  \n",
       "2                    0.253629  0.183333        1  \n",
       "3                    0.038002  0.000000        0  \n",
       "4                    0.943638  0.200000        1  \n",
       "..                        ...       ...      ...  \n",
       "763                  0.039710  0.700000        0  \n",
       "764                  0.111870  0.100000        0  \n",
       "765                  0.071307  0.150000        0  \n",
       "766                  0.115713  0.433333        1  \n",
       "767                  0.101196  0.033333        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/cleaned.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('Outcome', axis=1).values, df.Outcome.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=42)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "y_test = torch.FloatTensor(y_test).unsqueeze(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Our neural network consists of input layer (8 neurons), one hidden (4 neurons) and an output layer (1 neuron). All layers except output have ReLU activation function and the output layer has Sigmoid activation function, because we need the output to be in interval <0, 1>. Because we are doing binary classification, we used binary cross entropy loss (BCELoss) and as an optimizer we used Adam.\n",
    "\n",
    "Additionally, learning rate scheduler has been used to make the learning rate variable - higher at the beginning of the training and getting gradually smaller as we are nearing the minimum.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- epochs = 500\n",
    "- base learing rate = 0.001\n",
    "- max learning rate = 0.01\n",
    "- annealing strategy = linear\n",
    "- steps per epoch = 1\n",
    "\n",
    "### Experiments\n",
    "\n",
    "Link to wandb report: https://api.wandb.ai/links/nsiete-hrnciar-katkovcin/0haxn726\n",
    "\n",
    "1. **Bigger net** `(onecycle-cos-deeper)` - input layer (8 neurons), two hidden layers (16 and 8 neurons) and an output layer (1 neuron). Activation function ReLU on all layers except for output, which is ended with Sigmoid, BCELoss loss function, Adam optimizer, and OneCycle scheduler with _linear_ annealing strategy. This net led to overfitting, which is indicated by validation loss diverging from training loss - training loss decreases while validation loss increases (training accuracy increases while validation accuracy decreases).\n",
    "\n",
    "2. **Cyclic scheduler** `(cyclic-scheduler-triangular2)` - input layer (8 neurons), one hidden (4 neurons) and an output layer (1 neuron). Activations, loss and optimizer same as before, scheduler was changed to Cyclic scheduler with triangular2 strategy. This net didn't overfit, mostly due to smaller net, but it also had a smaller accuracy of 82% and higher loss.\n",
    "\n",
    "3. **No scheduler with learning rate 0.001** `(no-scheduler-0.001)` - same net without scheduler yielded higher loss and smaller accuracy of 77%. This may be due to small learning rate and small step which greatly increased the training time and the net could have ended in local minimum, or it didn't have enough time (epochs) to converge to good enough result.\n",
    "\n",
    "4. **No scheduler with learning rate 0.001** `(no-scheduler-0.001)` - parameters were the same as in previous experiment, but with higher learning rate. In this case net managed to converge to better result with 86% validation accuracy. It however led to overfitting which is indicated by diverging losses.\n",
    "\n",
    "5. **OneCycle scheduler with _cos_ annealing strategy** `(onecycle-cos-1)` - same net with OneCycle scheduler with _cos_ annealing strategy. This net ended up with 84% validation accuracy, but as before, it started to overfit. In the report, we can see that the learning rate was increased by the scheduler until the maximum and then gradually decreased until the end of the training.\n",
    "\n",
    "6. **OneCycle scheduler with _linear_ annealing strategy** `(onecycle-linear-1)` - same net with OneCycle scheduler with _linear_ annealing strategy. This net yielded with slightly better results than with _cos_ strategy, with 85% accuracy. It also didn't overfit, but ended up with higher loss than in the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (f_connected1): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (f_connected2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (out): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mhrnciar/Desktop/School/FIIT/Ing/2. Semester/Neurónové siete/Cvičenia/basic-nn/wandb/run-20230329_104837-onecycle-linear-1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-linear-1' target=\"_blank\">onecycle-linear-1</a></strong> to <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch' target=\"_blank\">https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-linear-1' target=\"_blank\">https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-linear-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=8, hidden2=4, out_features=1):\n",
    "        super().__init__()\n",
    "        self.f_connected1 = nn.Linear(input_features, hidden1)\n",
    "        self.f_connected2 = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.f_connected1(x))\n",
    "        x = self.relu(self.f_connected2(x))\n",
    "        x = self.sigmoid(self.out(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['torch'].getfloat('start_lr'))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=config['torch'].getfloat('max_lr'),\n",
    "                                                #base_lr=config['torch'].getfloat('start_lr'),\n",
    "                                                epochs=config['default'].getint('epochs'),\n",
    "                                                steps_per_epoch=1,\n",
    "                                                anneal_strategy=config['torch']['strategy'],\n",
    "                                                cycle_momentum=False)\n",
    "\n",
    "run = wandb.init(project=\"basic-nn-torch\", id=\"onecycle-linear-1\")\n",
    "wandb.config.update(config)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training is calculated loss and accuracy and the learning rate is adjusted by the scheduler. During testing is calculated the testing loss and accuracy, as well as F1-score, and all metrics are logged to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "----------------------------------------\n",
      "Training loss: 0.658810019493103, validation loss: 0.6616303324699402\n",
      "\n",
      "Epoch 20\n",
      "----------------------------------------\n",
      "Training loss: 0.656925618648529, validation loss: 0.6598610877990723\n",
      "\n",
      "Epoch 30\n",
      "----------------------------------------\n",
      "Training loss: 0.6543087363243103, validation loss: 0.657429575920105\n",
      "\n",
      "Epoch 40\n",
      "----------------------------------------\n",
      "Training loss: 0.6509592533111572, validation loss: 0.6544548869132996\n",
      "\n",
      "Epoch 50\n",
      "----------------------------------------\n",
      "Training loss: 0.6465943455696106, validation loss: 0.6506621837615967\n",
      "\n",
      "Epoch 60\n",
      "----------------------------------------\n",
      "Training loss: 0.6397914886474609, validation loss: 0.6443706154823303\n",
      "\n",
      "Epoch 70\n",
      "----------------------------------------\n",
      "Training loss: 0.6272427439689636, validation loss: 0.6323026418685913\n",
      "\n",
      "Epoch 80\n",
      "----------------------------------------\n",
      "Training loss: 0.6033155918121338, validation loss: 0.6105956435203552\n",
      "\n",
      "Epoch 90\n",
      "----------------------------------------\n",
      "Training loss: 0.5625115633010864, validation loss: 0.5758366584777832\n",
      "\n",
      "Epoch 100\n",
      "----------------------------------------\n",
      "Training loss: 0.5075090527534485, validation loss: 0.535267174243927\n",
      "\n",
      "Epoch 110\n",
      "----------------------------------------\n",
      "Training loss: 0.4598695933818817, validation loss: 0.5079259276390076\n",
      "\n",
      "Epoch 120\n",
      "----------------------------------------\n",
      "Training loss: 0.4399283826351166, validation loss: 0.49235984683036804\n",
      "\n",
      "Epoch 130\n",
      "----------------------------------------\n",
      "Training loss: 0.43218696117401123, validation loss: 0.47156965732574463\n",
      "\n",
      "Epoch 140\n",
      "----------------------------------------\n",
      "Training loss: 0.42430537939071655, validation loss: 0.45208290219306946\n",
      "\n",
      "Epoch 150\n",
      "----------------------------------------\n",
      "Training loss: 0.419640451669693, validation loss: 0.4444373846054077\n",
      "\n",
      "Epoch 160\n",
      "----------------------------------------\n",
      "Training loss: 0.4150409400463104, validation loss: 0.4434588551521301\n",
      "\n",
      "Epoch 170\n",
      "----------------------------------------\n",
      "Training loss: 0.41009536385536194, validation loss: 0.4405089020729065\n",
      "\n",
      "Epoch 180\n",
      "----------------------------------------\n",
      "Training loss: 0.4053008258342743, validation loss: 0.4366169273853302\n",
      "\n",
      "Epoch 190\n",
      "----------------------------------------\n",
      "Training loss: 0.3991145193576813, validation loss: 0.4281025230884552\n",
      "\n",
      "Epoch 200\n",
      "----------------------------------------\n",
      "Training loss: 0.39160773158073425, validation loss: 0.4232403039932251\n",
      "\n",
      "Epoch 210\n",
      "----------------------------------------\n",
      "Training loss: 0.3839893043041229, validation loss: 0.41547444462776184\n",
      "\n",
      "Epoch 220\n",
      "----------------------------------------\n",
      "Training loss: 0.3767184913158417, validation loss: 0.40909168124198914\n",
      "\n",
      "Epoch 230\n",
      "----------------------------------------\n",
      "Training loss: 0.3688884377479553, validation loss: 0.40198713541030884\n",
      "\n",
      "Epoch 240\n",
      "----------------------------------------\n",
      "Training loss: 0.36142903566360474, validation loss: 0.39531010389328003\n",
      "\n",
      "Epoch 250\n",
      "----------------------------------------\n",
      "Training loss: 0.3548453748226166, validation loss: 0.39071643352508545\n",
      "\n",
      "Epoch 260\n",
      "----------------------------------------\n",
      "Training loss: 0.3496153950691223, validation loss: 0.3881816267967224\n",
      "\n",
      "Epoch 270\n",
      "----------------------------------------\n",
      "Training loss: 0.34527552127838135, validation loss: 0.3906756043434143\n",
      "\n",
      "Epoch 280\n",
      "----------------------------------------\n",
      "Training loss: 0.3414941430091858, validation loss: 0.393157422542572\n",
      "\n",
      "Epoch 290\n",
      "----------------------------------------\n",
      "Training loss: 0.3384021818637848, validation loss: 0.39542219042778015\n",
      "\n",
      "Epoch 300\n",
      "----------------------------------------\n",
      "Training loss: 0.3358871638774872, validation loss: 0.3983137309551239\n",
      "\n",
      "Epoch 310\n",
      "----------------------------------------\n",
      "Training loss: 0.33374789357185364, validation loss: 0.3988686203956604\n",
      "\n",
      "Epoch 320\n",
      "----------------------------------------\n",
      "Training loss: 0.33207547664642334, validation loss: 0.40017151832580566\n",
      "\n",
      "Epoch 330\n",
      "----------------------------------------\n",
      "Training loss: 0.3306884467601776, validation loss: 0.40031710267066956\n",
      "\n",
      "Epoch 340\n",
      "----------------------------------------\n",
      "Training loss: 0.32952407002449036, validation loss: 0.4007634222507477\n",
      "\n",
      "Epoch 350\n",
      "----------------------------------------\n",
      "Training loss: 0.32851865887641907, validation loss: 0.4005592167377472\n",
      "\n",
      "Epoch 360\n",
      "----------------------------------------\n",
      "Training loss: 0.3276149332523346, validation loss: 0.4002341330051422\n",
      "\n",
      "Epoch 370\n",
      "----------------------------------------\n",
      "Training loss: 0.32673728466033936, validation loss: 0.3988817632198334\n",
      "\n",
      "Epoch 380\n",
      "----------------------------------------\n",
      "Training loss: 0.32592400908470154, validation loss: 0.3988587558269501\n",
      "\n",
      "Epoch 390\n",
      "----------------------------------------\n",
      "Training loss: 0.3251205086708069, validation loss: 0.3979514539241791\n",
      "\n",
      "Epoch 400\n",
      "----------------------------------------\n",
      "Training loss: 0.3243359923362732, validation loss: 0.39782100915908813\n",
      "\n",
      "Epoch 410\n",
      "----------------------------------------\n",
      "Training loss: 0.32371634244918823, validation loss: 0.39721348881721497\n",
      "\n",
      "Epoch 420\n",
      "----------------------------------------\n",
      "Training loss: 0.3231872320175171, validation loss: 0.39680156111717224\n",
      "\n",
      "Epoch 430\n",
      "----------------------------------------\n",
      "Training loss: 0.32274970412254333, validation loss: 0.39650726318359375\n",
      "\n",
      "Epoch 440\n",
      "----------------------------------------\n",
      "Training loss: 0.32235631346702576, validation loss: 0.39637985825538635\n",
      "\n",
      "Epoch 450\n",
      "----------------------------------------\n",
      "Training loss: 0.32202276587486267, validation loss: 0.3960460126399994\n",
      "\n",
      "Epoch 460\n",
      "----------------------------------------\n",
      "Training loss: 0.3216608762741089, validation loss: 0.3957688808441162\n",
      "\n",
      "Epoch 470\n",
      "----------------------------------------\n",
      "Training loss: 0.3213861286640167, validation loss: 0.3954964876174927\n",
      "\n",
      "Epoch 480\n",
      "----------------------------------------\n",
      "Training loss: 0.3212032616138458, validation loss: 0.39517924189567566\n",
      "\n",
      "Epoch 490\n",
      "----------------------------------------\n",
      "Training loss: 0.32109495997428894, validation loss: 0.3950386047363281\n",
      "\n",
      "Epoch 500\n",
      "----------------------------------------\n",
      "Training loss: 0.3210577070713043, validation loss: 0.39494651556015015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "\n",
    "for i in range(config['default'].getint('epochs')):\n",
    "    i += 1\n",
    "    y_pred = model.forward(X_train)\n",
    "    train_loss = loss_fn(y_pred, y_train)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    y_pred = (y_pred > 0.5).int()\n",
    "    train_accuracy = accuracy_score(y_train.squeeze(1).int(), y_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "    scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        val_loss = loss_fn(y_pred, y_test)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        y_pred = (y_pred > 0.5).int()\n",
    "        wandb.log({'training_loss': train_loss, 'validation_loss': val_loss}, commit=False)\n",
    "\n",
    "        f1_none = f1_score(y_test.squeeze(1).int(), y_pred, average=None)\n",
    "        f1_none = {'f1_none/' + str(e): v for e,v in enumerate(f1_none)}\n",
    "        wandb.log(f1_none, commit=False)\n",
    "\n",
    "        f1_macro = f1_score(y_test.squeeze(1).int(), y_pred, average='macro')\n",
    "        wandb.log({'f1_macro': f1_macro}, commit=False)\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_test.squeeze(1).int(), y_pred)\n",
    "        wandb.log({'train_accuracy': train_accuracy, 'val_accuracy': val_accuracy})\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}')\n",
    "        print('-' * 40)\n",
    "        print(f'Training loss: {train_loss}, validation loss: {val_loss}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>▁▁▁▁▁▁▁▃▆▆▆▇▇▇▇▇████████████████████████</td></tr><tr><td>f1_none/0</td><td>▁▁▁▁▁▁▁▂▂▂▃▄▄▄▅▆▇▆▆███▇▇▇▇▇██▇▇▇▇▇▇▇██▇▇</td></tr><tr><td>f1_none/1</td><td>▁▁▁▁▁▁▁▃▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>learning_rate</td><td>▁▂▂▃▄▄▅▅▆▇▇████▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>training_loss</td><td>██████▇▆▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▂▄▅▅▅▆▆▇▇▇▇▇█████▇▇███▇▇█████████</td></tr><tr><td>validation_loss</td><td>█████▇▇▆▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>0.83515</td></tr><tr><td>f1_none/0</td><td>0.87368</td></tr><tr><td>f1_none/1</td><td>0.79661</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_accuracy</td><td>0.86971</td></tr><tr><td>training_loss</td><td>0.32106</td></tr><tr><td>val_accuracy</td><td>0.84416</td></tr><tr><td>validation_loss</td><td>0.39495</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">onecycle-linear-1</strong> at: <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-linear-1' target=\"_blank\">https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-linear-1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230329_104837-onecycle-linear-1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/model.pth\")\n",
    "wandb.save('runs/pima_run_2023-03-29')\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of training, we can calculate the final testing accuracy - the last run yielded testing accuracy of about 85%. This accuracy is not the best, but with the size of the net we have, it is quite good. It could be increased by making the net bigger, but that also increases the risk of overfitting, so we would need to implement regularization or other method of overfit prevention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441558441558441"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_pred = model(data)\n",
    "        predictions.append((y_pred > 0.5).int().item())\n",
    "\n",
    "score = accuracy_score(y_test, predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
