{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhrnciar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(torch.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>169.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6    148.0           72.0           35.0    169.5  33.6   \n",
       "1              1     85.0           66.0           29.0    102.5  26.6   \n",
       "2              8    183.0           64.0           32.0    169.5  23.3   \n",
       "3              1     89.0           66.0           23.0     94.0  28.1   \n",
       "4              0    137.0           40.0           35.0    168.0  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10    101.0           76.0           48.0    180.0  32.9   \n",
       "764            2    122.0           70.0           27.0    102.5  36.8   \n",
       "765            5    121.0           72.0           23.0    112.0  26.2   \n",
       "766            1    126.0           60.0           32.0    169.5  30.1   \n",
       "767            1     93.0           70.0           31.0    102.5  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/cleaned.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.314928</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.171779</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.104294</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.185096</td>\n",
       "      <td>0.509202</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.367742</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.445652</td>\n",
       "      <td>0.199519</td>\n",
       "      <td>0.300613</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.503226</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.380368</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.496774</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.117788</td>\n",
       "      <td>0.163599</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.529032</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.243354</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.316129</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.249489</td>\n",
       "      <td>0.101196</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0       0.352941  0.670968       0.489796       0.304348  0.186899  0.314928   \n",
       "1       0.058824  0.264516       0.428571       0.239130  0.106370  0.171779   \n",
       "2       0.470588  0.896774       0.408163       0.271739  0.186899  0.104294   \n",
       "3       0.058824  0.290323       0.428571       0.173913  0.096154  0.202454   \n",
       "4       0.000000  0.600000       0.163265       0.304348  0.185096  0.509202   \n",
       "..           ...       ...            ...            ...       ...       ...   \n",
       "763     0.588235  0.367742       0.530612       0.445652  0.199519  0.300613   \n",
       "764     0.117647  0.503226       0.469388       0.217391  0.106370  0.380368   \n",
       "765     0.294118  0.496774       0.489796       0.173913  0.117788  0.163599   \n",
       "766     0.058824  0.529032       0.367347       0.271739  0.186899  0.243354   \n",
       "767     0.058824  0.316129       0.469388       0.260870  0.106370  0.249489   \n",
       "\n",
       "     DiabetesPedigreeFunction       Age  Outcome  \n",
       "0                    0.234415  0.483333        1  \n",
       "1                    0.116567  0.166667        0  \n",
       "2                    0.253629  0.183333        1  \n",
       "3                    0.038002  0.000000        0  \n",
       "4                    0.943638  0.200000        1  \n",
       "..                        ...       ...      ...  \n",
       "763                  0.039710  0.700000        0  \n",
       "764                  0.111870  0.100000        0  \n",
       "765                  0.071307  0.150000        0  \n",
       "766                  0.115713  0.433333        1  \n",
       "767                  0.101196  0.033333        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']] = minmax_scaling(df,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('Outcome', axis=1).values, df.Outcome.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=42)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "y_test = torch.FloatTensor(y_test).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (f_connected1): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (f_connected2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (out): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mhrnciar/Desktop/School/FIIT/Ing/2. Semester/Neurónové siete/Cvičenia/basic-nn/wandb/run-20230324_083725-vivid-serenity-16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-16' target=\"_blank\">vivid-serenity-16</a></strong> to <a href='https://wandb.ai/mhrnciar/basic-nn-torch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mhrnciar/basic-nn-torch' target=\"_blank\">https://wandb.ai/mhrnciar/basic-nn-torch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-16' target=\"_blank\">https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=8, hidden2=4, out_features=1):\n",
    "        super().__init__()\n",
    "        self.f_connected1 = nn.Linear(input_features, hidden1)\n",
    "        self.f_connected2 = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.f_connected1(x))\n",
    "        x = self.relu(self.f_connected2(x))\n",
    "        x = self.sigmoid(self.out(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['torch'].getfloat('start_lr'))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=config['torch'].getfloat('max_lr'),\n",
    "                                                steps_per_epoch=config['torch'].getint('steps_per_epoch'),\n",
    "                                                epochs=config['default'].getint('epochs'),\n",
    "                                                anneal_strategy=config['torch']['anneal_strategy'])\n",
    "\n",
    "run = wandb.init(project=\"basic-nn-torch\", id=\"vivid-serenity-16\")\n",
    "wandb.config.update(config)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "----------------------------------------\n",
      "Training loss: 0.6531457304954529, validation loss: 0.6574645638465881\n",
      "\n",
      "Epoch 20\n",
      "----------------------------------------\n",
      "Training loss: 0.6329783201217651, validation loss: 0.6426263451576233\n",
      "\n",
      "Epoch 30\n",
      "----------------------------------------\n",
      "Training loss: 0.600551426410675, validation loss: 0.6072851419448853\n",
      "\n",
      "Epoch 40\n",
      "----------------------------------------\n",
      "Training loss: 0.5408597588539124, validation loss: 0.5569384694099426\n",
      "\n",
      "Epoch 50\n",
      "----------------------------------------\n",
      "Training loss: 0.48255255818367004, validation loss: 0.530147910118103\n",
      "\n",
      "Epoch 60\n",
      "----------------------------------------\n",
      "Training loss: 0.4544718861579895, validation loss: 0.5439247488975525\n",
      "\n",
      "Epoch 70\n",
      "----------------------------------------\n",
      "Training loss: 0.43022871017456055, validation loss: 0.5004216432571411\n",
      "\n",
      "Epoch 80\n",
      "----------------------------------------\n",
      "Training loss: 0.4037347435951233, validation loss: 0.4344600439071655\n",
      "\n",
      "Epoch 90\n",
      "----------------------------------------\n",
      "Training loss: 0.37359684705734253, validation loss: 0.4109242260456085\n",
      "\n",
      "Epoch 100\n",
      "----------------------------------------\n",
      "Training loss: 0.36268919706344604, validation loss: 0.39843279123306274\n",
      "\n",
      "Epoch 110\n",
      "----------------------------------------\n",
      "Training loss: 0.3476966917514801, validation loss: 0.39702409505844116\n",
      "\n",
      "Epoch 120\n",
      "----------------------------------------\n",
      "Training loss: 0.32882940769195557, validation loss: 0.42269065976142883\n",
      "\n",
      "Epoch 130\n",
      "----------------------------------------\n",
      "Training loss: 0.32688820362091064, validation loss: 0.4261696934700012\n",
      "\n",
      "Epoch 140\n",
      "----------------------------------------\n",
      "Training loss: 0.31338053941726685, validation loss: 0.4120457172393799\n",
      "\n",
      "Epoch 150\n",
      "----------------------------------------\n",
      "Training loss: 0.3053159713745117, validation loss: 0.42689138650894165\n",
      "\n",
      "Epoch 160\n",
      "----------------------------------------\n",
      "Training loss: 0.30141371488571167, validation loss: 0.4338361918926239\n",
      "\n",
      "Epoch 170\n",
      "----------------------------------------\n",
      "Training loss: 0.2989269196987152, validation loss: 0.4338407516479492\n",
      "\n",
      "Epoch 180\n",
      "----------------------------------------\n",
      "Training loss: 0.29782113432884216, validation loss: 0.4393745958805084\n",
      "\n",
      "Epoch 190\n",
      "----------------------------------------\n",
      "Training loss: 0.29791149497032166, validation loss: 0.4472694396972656\n",
      "\n",
      "Epoch 200\n",
      "----------------------------------------\n",
      "Training loss: 0.2965814471244812, validation loss: 0.4432271122932434\n",
      "\n",
      "Epoch 210\n",
      "----------------------------------------\n",
      "Training loss: 0.29513633251190186, validation loss: 0.44256800413131714\n",
      "\n",
      "Epoch 220\n",
      "----------------------------------------\n",
      "Training loss: 0.29380255937576294, validation loss: 0.4465843439102173\n",
      "\n",
      "Epoch 230\n",
      "----------------------------------------\n",
      "Training loss: 0.292153924703598, validation loss: 0.44848331809043884\n",
      "\n",
      "Epoch 240\n",
      "----------------------------------------\n",
      "Training loss: 0.29113373160362244, validation loss: 0.44131991267204285\n",
      "\n",
      "Epoch 250\n",
      "----------------------------------------\n",
      "Training loss: 0.2915648818016052, validation loss: 0.446367472410202\n",
      "\n",
      "Epoch 260\n",
      "----------------------------------------\n",
      "Training loss: 0.29034489393234253, validation loss: 0.4412800669670105\n",
      "\n",
      "Epoch 270\n",
      "----------------------------------------\n",
      "Training loss: 0.28960108757019043, validation loss: 0.4388412833213806\n",
      "\n",
      "Epoch 280\n",
      "----------------------------------------\n",
      "Training loss: 0.2894594371318817, validation loss: 0.43671852350234985\n",
      "\n",
      "Epoch 290\n",
      "----------------------------------------\n",
      "Training loss: 0.28841519355773926, validation loss: 0.4336113929748535\n",
      "\n",
      "Epoch 300\n",
      "----------------------------------------\n",
      "Training loss: 0.28830742835998535, validation loss: 0.43186327815055847\n",
      "\n",
      "Epoch 310\n",
      "----------------------------------------\n",
      "Training loss: 0.28786909580230713, validation loss: 0.43071964383125305\n",
      "\n",
      "Epoch 320\n",
      "----------------------------------------\n",
      "Training loss: 0.28771454095840454, validation loss: 0.4319089353084564\n",
      "\n",
      "Epoch 330\n",
      "----------------------------------------\n",
      "Training loss: 0.28708094358444214, validation loss: 0.4338599443435669\n",
      "\n",
      "Epoch 340\n",
      "----------------------------------------\n",
      "Training loss: 0.2866858243942261, validation loss: 0.4333028793334961\n",
      "\n",
      "Epoch 350\n",
      "----------------------------------------\n",
      "Training loss: 0.28653740882873535, validation loss: 0.4323134124279022\n",
      "\n",
      "Epoch 360\n",
      "----------------------------------------\n",
      "Training loss: 0.2863211929798126, validation loss: 0.4319475293159485\n",
      "\n",
      "Epoch 370\n",
      "----------------------------------------\n",
      "Training loss: 0.2861805856227875, validation loss: 0.4329560399055481\n",
      "\n",
      "Epoch 380\n",
      "----------------------------------------\n",
      "Training loss: 0.28605568408966064, validation loss: 0.4331318736076355\n",
      "\n",
      "Epoch 390\n",
      "----------------------------------------\n",
      "Training loss: 0.28582775592803955, validation loss: 0.4329269230365753\n",
      "\n",
      "Epoch 400\n",
      "----------------------------------------\n",
      "Training loss: 0.2853812575340271, validation loss: 0.43280860781669617\n",
      "\n",
      "Epoch 410\n",
      "----------------------------------------\n",
      "Training loss: 0.28501155972480774, validation loss: 0.4321630001068115\n",
      "\n",
      "Epoch 420\n",
      "----------------------------------------\n",
      "Training loss: 0.28471359610557556, validation loss: 0.4319210350513458\n",
      "\n",
      "Epoch 430\n",
      "----------------------------------------\n",
      "Training loss: 0.2845213711261749, validation loss: 0.431374728679657\n",
      "\n",
      "Epoch 440\n",
      "----------------------------------------\n",
      "Training loss: 0.2844337224960327, validation loss: 0.4311925172805786\n",
      "\n",
      "Epoch 450\n",
      "----------------------------------------\n",
      "Training loss: 0.2843817174434662, validation loss: 0.4312703013420105\n",
      "\n",
      "Epoch 460\n",
      "----------------------------------------\n",
      "Training loss: 0.2843468487262726, validation loss: 0.4314427673816681\n",
      "\n",
      "Epoch 470\n",
      "----------------------------------------\n",
      "Training loss: 0.2843242883682251, validation loss: 0.4316929876804352\n",
      "\n",
      "Epoch 480\n",
      "----------------------------------------\n",
      "Training loss: 0.28431349992752075, validation loss: 0.4317929744720459\n",
      "\n",
      "Epoch 490\n",
      "----------------------------------------\n",
      "Training loss: 0.28430962562561035, validation loss: 0.43180811405181885\n",
      "\n",
      "Epoch 500\n",
      "----------------------------------------\n",
      "Training loss: 0.2843087315559387, validation loss: 0.43180787563323975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "\n",
    "for i in range(config['default'].getint('epochs')):\n",
    "    i += 1\n",
    "    y_pred = model.forward(X_train)\n",
    "    train_loss = loss_fn(y_pred, y_train)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    y_pred = (y_pred > 0.5).int()\n",
    "    train_accuracy = accuracy_score(y_train.squeeze(1).int(), y_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "    scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        val_loss = loss_fn(y_pred, y_test)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        y_pred = (y_pred > 0.5).int()\n",
    "        wandb.log({'training_loss': train_loss, 'validation_loss': val_loss}, commit=False)\n",
    "\n",
    "        f1_none = f1_score(y_test.squeeze(1).int(), y_pred, average=None)\n",
    "        f1_none = {'f1_none/' + str(e): v for e,v in enumerate(f1_none)}\n",
    "        wandb.log(f1_none, commit=False)\n",
    "\n",
    "        f1_macro = f1_score(y_test.squeeze(1).int(), y_pred, average='macro')\n",
    "        wandb.log({'f1_macro': f1_macro}, commit=False)\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_test.squeeze(1).int(), y_pred)\n",
    "        wandb.log({'train_accuracy': train_accuracy, 'val_accuracy': val_accuracy})\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}')\n",
    "        print('-' * 40)\n",
    "        print(f'Training loss: {train_loss}, validation loss: {val_loss}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "train",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.6681301593780518,
          0.6664175987243652,
          0.6647412180900574,
          0.6631007790565491,
          0.6614874601364136,
          0.6598727703094482,
          0.6582474708557129,
          0.6565804481506348,
          0.6548903584480286,
          0.6531457304954529,
          0.6513288617134094,
          0.6494618058204651,
          0.647554874420166,
          0.645595908164978,
          0.6435980796813965,
          0.6415368914604187,
          0.6394343972206116,
          0.6373088955879211,
          0.6351568698883057,
          0.6329783201217651,
          0.6307298541069031,
          0.6283985376358032,
          0.6259506940841675,
          0.6232832074165344,
          0.6204162240028381,
          0.6172343492507935,
          0.6136793494224548,
          0.6097041368484497,
          0.6053057312965393,
          0.600551426410675,
          0.5954084396362305,
          0.5902045965194702,
          0.5850537419319153,
          0.5796671509742737,
          0.5738533139228821,
          0.5675878524780273,
          0.5608888864517212,
          0.5539581179618835,
          0.5472463965415955,
          0.5408597588539124,
          0.5347003936767578,
          0.5282995104789734,
          0.5214765071868896,
          0.5147718787193298,
          0.5084361433982849,
          0.502886176109314,
          0.4973888695240021,
          0.49169644713401794,
          0.4866062104701996,
          0.48255255818367004,
          0.4787161350250244,
          0.4747012257575989,
          0.47112515568733215,
          0.4683072566986084,
          0.4651765525341034,
          0.46238380670547485,
          0.4603281319141388,
          0.4581221640110016,
          0.4565275013446808,
          0.4544718861579895,
          0.4524969458580017,
          0.45051687955856323,
          0.44797638058662415,
          0.4458049535751343,
          0.44288119673728943,
          0.44059062004089355,
          0.43778130412101746,
          0.43536362051963806,
          0.43282121419906616,
          0.43022871017456055,
          0.4272892475128174,
          0.4243339002132416,
          0.42092767357826233,
          0.4175562560558319,
          0.4151340425014496,
          0.4132363796234131,
          0.4112274944782257,
          0.4087701737880707,
          0.4064064919948578,
          0.4037347435951233,
          0.40048864483833313,
          0.39692485332489014,
          0.3938654661178589,
          0.393724262714386,
          0.3974520266056061,
          0.3930830657482147,
          0.3812057673931122,
          0.3836914002895355,
          0.3826068937778473,
          0.37359684705734253,
          0.37491559982299805,
          0.3800043761730194,
          0.3720288872718811,
          0.3647177517414093,
          0.3706504702568054,
          0.3678896427154541,
          0.3604887127876282,
          0.3608199656009674,
          0.3639117181301117,
          0.36268919706344604,
          0.3559960126876831,
          0.35458114743232727,
          0.357142835855484,
          0.35623618960380554,
          0.3525770604610443,
          0.3482274115085602,
          0.3477574288845062,
          0.34879663586616516,
          0.34819743037223816,
          0.3476966917514801,
          0.3452950716018677,
          0.34362196922302246,
          0.3374989628791809,
          0.3329200744628906,
          0.3306833803653717,
          0.3320707380771637,
          0.3393898010253906,
          0.3638738691806793,
          0.39858636260032654,
          0.32882940769195557,
          0.3622519373893738,
          0.38333866000175476,
          0.32438358664512634,
          0.39922624826431274,
          0.368375688791275,
          0.33748361468315125,
          0.4029223322868347,
          0.3284654915332794,
          0.35241469740867615,
          0.32688820362091064,
          0.33952048420906067,
          0.3299025595188141,
          0.3225322961807251,
          0.33628255128860474,
          0.3142096996307373,
          0.3257264196872711,
          0.3130803108215332,
          0.3225463032722473,
          0.3161728084087372,
          0.31338053941726685,
          0.3165353238582611,
          0.3087042570114136,
          0.3133408725261688,
          0.30797451734542847,
          0.3087283670902252,
          0.30968886613845825,
          0.3057529032230377,
          0.30797266960144043,
          0.3055932819843292,
          0.3053159713745117,
          0.3059932291507721,
          0.30332210659980774,
          0.3040529787540436,
          0.30363351106643677,
          0.3021390736103058,
          0.3027777373790741,
          0.30221638083457947,
          0.3012465834617615,
          0.30165502429008484,
          0.30141371488571167,
          0.3005707859992981,
          0.3005959689617157,
          0.30048707127571106,
          0.30001211166381836,
          0.2997894883155823,
          0.2996465861797333,
          0.299577921628952,
          0.2993716895580292,
          0.29917654395103455,
          0.2989269196987152,
          0.29885604977607727,
          0.2988811731338501,
          0.29910650849342346,
          0.29911550879478455,
          0.29881277680397034,
          0.29842373728752136,
          0.2982420325279236,
          0.2980618178844452,
          0.2980569303035736,
          0.29782113432884216,
          0.2976982891559601,
          0.29759523272514343,
          0.29739663004875183,
          0.297282874584198,
          0.29710260033607483,
          0.29705601930618286,
          0.29702407121658325,
          0.2972160279750824,
          0.2977496087551117,
          0.29791149497032166,
          0.2982880473136902,
          0.29813840985298157,
          0.29746270179748535,
          0.29675599932670593,
          0.29651349782943726,
          0.2963045537471771,
          0.29616275429725647,
          0.2964022159576416,
          0.2962697744369507,
          0.2965814471244812,
          0.2965010404586792,
          0.29667678475379944,
          0.2961289584636688,
          0.2956060469150543,
          0.29512256383895874,
          0.295090913772583,
          0.2957296073436737,
          0.2976081371307373,
          0.29784661531448364,
          0.29513633251190186,
          0.2950800359249115,
          0.29829058051109314,
          0.30024978518486023,
          0.2992921471595764,
          0.29543033242225647,
          0.2943820059299469,
          0.29606127738952637,
          0.2961794137954712,
          0.29444122314453125,
          0.29380255937576294,
          0.2937726080417633,
          0.29360032081604004,
          0.29389387369155884,
          0.2934143543243408,
          0.29305338859558105,
          0.2930748760700226,
          0.2925398647785187,
          0.29248514771461487,
          0.2923984229564667,
          0.292153924703598,
          0.29221808910369873,
          0.29199710488319397,
          0.2916894853115082,
          0.29151877760887146,
          0.29141324758529663,
          0.29176658391952515,
          0.29198509454727173,
          0.2919621765613556,
          0.29122498631477356,
          0.29113373160362244,
          0.2908206880092621,
          0.2908327281475067,
          0.29079633951187134,
          0.29067257046699524,
          0.29060298204421997,
          0.2906162738800049,
          0.2906588912010193,
          0.29138103127479553,
          0.29181987047195435,
          0.2915648818016052,
          0.2914203703403473,
          0.2904352843761444,
          0.2911141514778137,
          0.29120779037475586,
          0.2908223569393158,
          0.2906365394592285,
          0.29050910472869873,
          0.29051560163497925,
          0.2904873192310333,
          0.29034489393234253,
          0.2903170585632324,
          0.290241539478302,
          0.2901640236377716,
          0.29002609848976135,
          0.29004183411598206,
          0.29055628180503845,
          0.2901962399482727,
          0.28993549942970276,
          0.2898101806640625,
          0.28960108757019043,
          0.2895757853984833,
          0.2896409332752228,
          0.2893676161766052,
          0.28926774859428406,
          0.28924423456192017,
          0.2892317473888397,
          0.28941547870635986,
          0.2890743911266327,
          0.28968334197998047,
          0.2894594371318817,
          0.28895407915115356,
          0.2888289988040924,
          0.2892085909843445,
          0.288696825504303,
          0.28856536746025085,
          0.2886764705181122,
          0.2886717915534973,
          0.28862234950065613,
          0.28860750794410706,
          0.28841519355773926,
          0.2887458801269531,
          0.2883579432964325,
          0.28854429721832275,
          0.28825730085372925,
          0.28844088315963745,
          0.28820860385894775,
          0.2883926033973694,
          0.2883392870426178,
          0.28817006945610046,
          0.28830742835998535,
          0.28806471824645996,
          0.2883729636669159,
          0.2881093919277191,
          0.28823381662368774,
          0.28841257095336914,
          0.2884135842323303,
          0.28796690702438354,
          0.2882881760597229,
          0.28821587562561035,
          0.28786909580230713,
          0.2879549562931061,
          0.2878880798816681,
          0.28783485293388367,
          0.2878332734107971,
          0.2878594398498535,
          0.28778234124183655,
          0.2877540588378906,
          0.2878492474555969,
          0.2876882553100586,
          0.28771454095840454,
          0.2876635789871216,
          0.2875261604785919,
          0.28747716546058655,
          0.2873678207397461,
          0.28729912638664246,
          0.2872260808944702,
          0.28720715641975403,
          0.28713399171829224,
          0.2871025800704956,
          0.28708094358444214,
          0.2869819104671478,
          0.28696784377098083,
          0.2868892252445221,
          0.28691694140434265,
          0.28687360882759094,
          0.28688663244247437,
          0.2868545353412628,
          0.28680649399757385,
          0.28679418563842773,
          0.2866858243942261,
          0.28672829270362854,
          0.28672662377357483,
          0.28660544753074646,
          0.28664830327033997,
          0.28665101528167725,
          0.28653213381767273,
          0.28658562898635864,
          0.2865470349788666,
          0.28647324442863464,
          0.28653740882873535,
          0.28640079498291016,
          0.28643620014190674,
          0.2865035831928253,
          0.2864617705345154,
          0.286357581615448,
          0.2863852083683014,
          0.28631019592285156,
          0.2863197922706604,
          0.28631147742271423,
          0.2863211929798126,
          0.28634315729141235,
          0.2863169014453888,
          0.28627869486808777,
          0.2862492799758911,
          0.286268413066864,
          0.2862718999385834,
          0.2862585783004761,
          0.28620806336402893,
          0.2861962914466858,
          0.2861805856227875,
          0.28617796301841736,
          0.2861352860927582,
          0.2861452102661133,
          0.2861270010471344,
          0.2861241400241852,
          0.2861238718032837,
          0.2860889732837677,
          0.2860938608646393,
          0.2860615849494934,
          0.28605568408966064,
          0.28604012727737427,
          0.2860361337661743,
          0.2860192656517029,
          0.2860191762447357,
          0.28600773215293884,
          0.2859945297241211,
          0.2859737277030945,
          0.2859066128730774,
          0.28587305545806885,
          0.28582775592803955,
          0.2857874631881714,
          0.2857414782047272,
          0.2856998145580292,
          0.2856513559818268,
          0.28561848402023315,
          0.28555893898010254,
          0.2855137586593628,
          0.2854660749435425,
          0.28543081879615784,
          0.2853812575340271,
          0.2853289544582367,
          0.2852952480316162,
          0.28526178002357483,
          0.2852140963077545,
          0.28517836332321167,
          0.285145103931427,
          0.2851025462150574,
          0.2850700616836548,
          0.2850416302680969,
          0.28501155972480774,
          0.28497815132141113,
          0.2849400043487549,
          0.2849039137363434,
          0.28488093614578247,
          0.2848531901836395,
          0.28482434153556824,
          0.28479236364364624,
          0.2847692370414734,
          0.284742146730423,
          0.28471359610557556,
          0.2846915125846863,
          0.2846655547618866,
          0.28464365005493164,
          0.28462615609169006,
          0.2846076786518097,
          0.28458619117736816,
          0.28456851840019226,
          0.2845580279827118,
          0.28454044461250305,
          0.2845213711261749,
          0.2845070958137512,
          0.2844926118850708,
          0.28448066115379333,
          0.28447043895721436,
          0.2844645380973816,
          0.2844560146331787,
          0.28444769978523254,
          0.2844378352165222,
          0.2844369411468506,
          0.2844337224960327,
          0.28442588448524475,
          0.28441643714904785,
          0.2844091057777405,
          0.28441035747528076,
          0.2844041585922241,
          0.28439146280288696,
          0.2843906879425049,
          0.28438934683799744,
          0.28438636660575867,
          0.2843817174434662,
          0.28437334299087524,
          0.2843688130378723,
          0.2843691408634186,
          0.28436484932899475,
          0.2843610942363739,
          0.2843548655509949,
          0.28435418009757996,
          0.2843530476093292,
          0.28435054421424866,
          0.2843468487262726,
          0.28434741497039795,
          0.2843441069126129,
          0.2843371331691742,
          0.2843327820301056,
          0.2843349874019623,
          0.28433552384376526,
          0.28433430194854736,
          0.28433099389076233,
          0.2843267321586609,
          0.2843242883682251,
          0.28432297706604004,
          0.284321665763855,
          0.2843218147754669,
          0.2843216061592102,
          0.28432074189186096,
          0.2843184173107147,
          0.28431645035743713,
          0.28431546688079834,
          0.28431451320648193,
          0.28431349992752075,
          0.28431329131126404,
          0.2843129336833954,
          0.28431251645088196,
          0.28431200981140137,
          0.2843114733695984,
          0.2843110263347626,
          0.28431054949760437,
          0.2843100428581238,
          0.2843097746372223,
          0.28430962562561035,
          0.2843095064163208,
          0.2843092978000641,
          0.28430917859077454,
          0.2843090891838074,
          0.28430894017219543,
          0.28430885076522827,
          0.2843088209629059,
          0.2843087613582611,
          0.2843087315559387,
          0.2843087315559387
         ]
        },
        {
         "name": "validation",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.668283998966217,
          0.6669235825538635,
          0.6656352877616882,
          0.6644190549850464,
          0.6632735729217529,
          0.6621882915496826,
          0.661110520362854,
          0.6599219441413879,
          0.6587237119674683,
          0.6574645638465881,
          0.656125545501709,
          0.6547032594680786,
          0.6533017158508301,
          0.6518912315368652,
          0.6504362225532532,
          0.648972749710083,
          0.6474999189376831,
          0.6460164785385132,
          0.6443750262260437,
          0.6426263451576233,
          0.6406421661376953,
          0.6383485198020935,
          0.6357836127281189,
          0.6329280138015747,
          0.6297101974487305,
          0.6260759234428406,
          0.6219729781150818,
          0.6174551248550415,
          0.612555742263794,
          0.6072851419448853,
          0.6022513508796692,
          0.5973175168037415,
          0.5925945043563843,
          0.5876403450965881,
          0.5823090672492981,
          0.5768987536430359,
          0.5715154409408569,
          0.566625714302063,
          0.5618614554405212,
          0.5569384694099426,
          0.5519658923149109,
          0.5467262268066406,
          0.5427830815315247,
          0.5397316217422485,
          0.5368871092796326,
          0.5338963270187378,
          0.5313088297843933,
          0.5302603840827942,
          0.5302615761756897,
          0.530147910118103,
          0.5290665626525879,
          0.5277391672134399,
          0.5275087952613831,
          0.5294620394706726,
          0.5341331958770752,
          0.5379352569580078,
          0.5378247499465942,
          0.53694748878479,
          0.5394009947776794,
          0.5439247488975525,
          0.545414924621582,
          0.5403181910514832,
          0.5359354615211487,
          0.5361100435256958,
          0.5337221026420593,
          0.5271815061569214,
          0.5214556455612183,
          0.5155901312828064,
          0.5083473920822144,
          0.5004216432571411,
          0.4918031692504883,
          0.48191139101982117,
          0.4713669419288635,
          0.46314746141433716,
          0.45631080865859985,
          0.4509375989437103,
          0.4468441307544708,
          0.4423026144504547,
          0.43989330530166626,
          0.4344600439071655,
          0.4358749985694885,
          0.4267652928829193,
          0.44244271516799927,
          0.42332136631011963,
          0.4416581094264984,
          0.4234614670276642,
          0.41852930188179016,
          0.4303523898124695,
          0.41456687450408936,
          0.4109242260456085,
          0.4333999454975128,
          0.40943193435668945,
          0.4097360670566559,
          0.42126336693763733,
          0.40634608268737793,
          0.4046069383621216,
          0.40642261505126953,
          0.4005765914916992,
          0.4125710129737854,
          0.39843279123306274,
          0.3981422185897827,
          0.40757209062576294,
          0.39732351899147034,
          0.4014660120010376,
          0.393999308347702,
          0.3929719924926758,
          0.40220507979393005,
          0.39551857113838196,
          0.4076613783836365,
          0.39702409505844116,
          0.4051602780818939,
          0.3931470513343811,
          0.3930746018886566,
          0.3937433958053589,
          0.39302027225494385,
          0.4157705307006836,
          0.4155927300453186,
          0.49755793809890747,
          0.40632563829421997,
          0.42269065976142883,
          0.49039262533187866,
          0.415377140045166,
          0.457103967666626,
          0.4659093916416168,
          0.422971248626709,
          0.4387373924255371,
          0.41848042607307434,
          0.4557211697101593,
          0.4187491834163666,
          0.4261696934700012,
          0.42883121967315674,
          0.4178629219532013,
          0.4035263955593109,
          0.4052908420562744,
          0.4328285753726959,
          0.4149157404899597,
          0.4226754903793335,
          0.43226009607315063,
          0.42795997858047485,
          0.4120457172393799,
          0.4154106378555298,
          0.43160372972488403,
          0.42054787278175354,
          0.42369189858436584,
          0.4363754093647003,
          0.4289734661579132,
          0.4245019853115082,
          0.43068793416023254,
          0.43207821249961853,
          0.42689138650894165,
          0.43278318643569946,
          0.43641841411590576,
          0.43196773529052734,
          0.43319785594940186,
          0.43563932180404663,
          0.4304538667201996,
          0.4332861602306366,
          0.43745511770248413,
          0.4322974979877472,
          0.4338361918926239,
          0.4338378310203552,
          0.4299122095108032,
          0.4319329559803009,
          0.4323838949203491,
          0.4316602051258087,
          0.4343316853046417,
          0.4304004907608032,
          0.43261411786079407,
          0.431338369846344,
          0.4338407516479492,
          0.43194708228111267,
          0.43720752000808716,
          0.4319708049297333,
          0.4366234242916107,
          0.43330660462379456,
          0.4373358190059662,
          0.43601131439208984,
          0.43875816464424133,
          0.43625879287719727,
          0.4393745958805084,
          0.43838611245155334,
          0.43931055068969727,
          0.4376494288444519,
          0.4388684332370758,
          0.4388035833835602,
          0.4402735233306885,
          0.438715398311615,
          0.4459531307220459,
          0.439349889755249,
          0.4472694396972656,
          0.438043475151062,
          0.44574984908103943,
          0.4403729736804962,
          0.44483482837677,
          0.44352978467941284,
          0.4415458142757416,
          0.44421377778053284,
          0.4424631595611572,
          0.4477977454662323,
          0.4432271122932434,
          0.44576358795166016,
          0.44038984179496765,
          0.4437029957771301,
          0.44379162788391113,
          0.4423007667064667,
          0.4462105929851532,
          0.442013680934906,
          0.45002254843711853,
          0.44295814633369446,
          0.44256800413131714,
          0.4492577910423279,
          0.44446513056755066,
          0.4526108205318451,
          0.445789098739624,
          0.4460298717021942,
          0.4480055272579193,
          0.44458988308906555,
          0.4459714889526367,
          0.4450964033603668,
          0.4465843439102173,
          0.4472752809524536,
          0.44566622376441956,
          0.4480184018611908,
          0.44946393370628357,
          0.45136988162994385,
          0.4496779441833496,
          0.448386013507843,
          0.4487619400024414,
          0.4475826323032379,
          0.44848331809043884,
          0.44650933146476746,
          0.44696906208992004,
          0.44535133242607117,
          0.44429224729537964,
          0.4467579126358032,
          0.44528982043266296,
          0.4468795955181122,
          0.4421239495277405,
          0.4417974352836609,
          0.44131991267204285,
          0.44151172041893005,
          0.44196972250938416,
          0.4414585530757904,
          0.4417957067489624,
          0.44162699580192566,
          0.44352731108665466,
          0.4411700963973999,
          0.4450152516365051,
          0.44239726662635803,
          0.446367472410202,
          0.4439561665058136,
          0.44126951694488525,
          0.4448612332344055,
          0.4440414011478424,
          0.4455268085002899,
          0.44368115067481995,
          0.4433945119380951,
          0.4446979761123657,
          0.4435245096683502,
          0.4412800669670105,
          0.44152286648750305,
          0.44192999601364136,
          0.44117408990859985,
          0.440610408782959,
          0.4380531311035156,
          0.4413759410381317,
          0.440967857837677,
          0.44160595536231995,
          0.43984708189964294,
          0.4388412833213806,
          0.43993085622787476,
          0.439047634601593,
          0.4382588863372803,
          0.438438355922699,
          0.4372387230396271,
          0.43870288133621216,
          0.43722212314605713,
          0.4342690408229828,
          0.4369008541107178,
          0.43671852350234985,
          0.4370628893375397,
          0.4378967881202698,
          0.43545717000961304,
          0.43555763363838196,
          0.43600398302078247,
          0.4337776005268097,
          0.43645116686820984,
          0.4364221394062042,
          0.4342065155506134,
          0.4336113929748535,
          0.43522030115127563,
          0.4345869719982147,
          0.43469080328941345,
          0.4336206018924713,
          0.4335411787033081,
          0.4335736930370331,
          0.4341345429420471,
          0.43287935853004456,
          0.431132972240448,
          0.43186327815055847,
          0.431721568107605,
          0.4318038523197174,
          0.43251869082450867,
          0.43179357051849365,
          0.4301680624485016,
          0.43076810240745544,
          0.4312363564968109,
          0.4310031235218048,
          0.43067193031311035,
          0.43071964383125305,
          0.43047457933425903,
          0.43132656812667847,
          0.4324954152107239,
          0.43303290009498596,
          0.4322206974029541,
          0.4315793812274933,
          0.43129169940948486,
          0.43173614144325256,
          0.43213337659835815,
          0.4319089353084564,
          0.43123120069503784,
          0.4312504827976227,
          0.43165042996406555,
          0.4323665201663971,
          0.432494580745697,
          0.4319233000278473,
          0.4327160716056824,
          0.43357720971107483,
          0.43398717045783997,
          0.4338599443435669,
          0.43264147639274597,
          0.4328577518463135,
          0.43363767862319946,
          0.43404895067214966,
          0.43407243490219116,
          0.4329902231693268,
          0.43173494935035706,
          0.4312700927257538,
          0.43218502402305603,
          0.4333028793334961,
          0.43319255113601685,
          0.43226876854896545,
          0.4315407872200012,
          0.43199628591537476,
          0.4328426122665405,
          0.43300291895866394,
          0.4331751763820648,
          0.4329325556755066,
          0.43257132172584534,
          0.4323134124279022,
          0.43254777789115906,
          0.43326613306999207,
          0.43297016620635986,
          0.43204230070114136,
          0.4317057132720947,
          0.4322514832019806,
          0.43229666352272034,
          0.43171191215515137,
          0.43161123991012573,
          0.4319475293159485,
          0.4320899248123169,
          0.4321615993976593,
          0.4323444664478302,
          0.43275877833366394,
          0.4332424998283386,
          0.43342098593711853,
          0.43307530879974365,
          0.43257513642311096,
          0.43274104595184326,
          0.4329560399055481,
          0.43317511677742004,
          0.43314024806022644,
          0.4333333671092987,
          0.43350812792778015,
          0.43350547552108765,
          0.43321308493614197,
          0.4329613447189331,
          0.4330689013004303,
          0.43322402238845825,
          0.4331318736076355,
          0.4328954219818115,
          0.43250221014022827,
          0.43231016397476196,
          0.43248292803764343,
          0.43268197774887085,
          0.43267157673835754,
          0.43242383003234863,
          0.4322552978992462,
          0.4326135814189911,
          0.4329269230365753,
          0.43312087655067444,
          0.4331965148448944,
          0.43309536576271057,
          0.4330207407474518,
          0.4331529438495636,
          0.43304428458213806,
          0.43289461731910706,
          0.4328514337539673,
          0.4328380525112152,
          0.43280860781669617,
          0.43272078037261963,
          0.4325856864452362,
          0.4325005114078522,
          0.4325644373893738,
          0.4326901137828827,
          0.43268120288848877,
          0.43251335620880127,
          0.43229302763938904,
          0.4322074353694916,
          0.4321630001068115,
          0.4321538805961609,
          0.4321436285972595,
          0.432054728269577,
          0.4319494664669037,
          0.43186429142951965,
          0.4317956268787384,
          0.4317223131656647,
          0.43175646662712097,
          0.43188074231147766,
          0.4319210350513458,
          0.43188396096229553,
          0.4318041205406189,
          0.43169185519218445,
          0.43165677785873413,
          0.4316501319408417,
          0.4317081570625305,
          0.431679904460907,
          0.43157482147216797,
          0.43146201968193054,
          0.431374728679657,
          0.4313148260116577,
          0.431302011013031,
          0.4313698709011078,
          0.4314064383506775,
          0.43139588832855225,
          0.43134045600891113,
          0.4312841594219208,
          0.43124300241470337,
          0.43120524287223816,
          0.4311925172805786,
          0.43120262026786804,
          0.4312637150287628,
          0.43132856488227844,
          0.4313264787197113,
          0.43126603960990906,
          0.43120238184928894,
          0.4312065839767456,
          0.4312233328819275,
          0.4312485456466675,
          0.4312703013420105,
          0.4313013553619385,
          0.43136003613471985,
          0.43142378330230713,
          0.43145525455474854,
          0.4314602017402649,
          0.4314415752887726,
          0.43143364787101746,
          0.43143177032470703,
          0.4314349889755249,
          0.4314427673816681,
          0.43148016929626465,
          0.43154239654541016,
          0.4316258728504181,
          0.4316937327384949,
          0.43173688650131226,
          0.4317583739757538,
          0.43175700306892395,
          0.43173980712890625,
          0.43171781301498413,
          0.4316929876804352,
          0.43167367577552795,
          0.43165865540504456,
          0.431654155254364,
          0.43166330456733704,
          0.43168362975120544,
          0.4317089021205902,
          0.4317334294319153,
          0.43175625801086426,
          0.43177735805511475,
          0.4317929744720459,
          0.4318040609359741,
          0.43181145191192627,
          0.4318157136440277,
          0.4318176805973053,
          0.4318177402019501,
          0.4318163990974426,
          0.43181413412094116,
          0.43181130290031433,
          0.43180936574935913,
          0.43180811405181885,
          0.4318074882030487,
          0.4318072199821472,
          0.4318072199821472,
          0.43180736899375916,
          0.4318075180053711,
          0.43180766701698303,
          0.43180781602859497,
          0.43180787563323975,
          0.43180787563323975,
          0.43180787563323975
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=list(range(config['default'].getint('epochs'))), y=list(map(lambda x: x.item(), train_losses)), name='train')\n",
    "fig.add_scatter(x=list(range(config['default'].getint('epochs'))), y=list(map(lambda x: x.item(), val_losses)), name='validation')\n",
    "fig.update_layout(xaxis_title='Epoch', yaxis_title='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>▁▁▁▁▆▆▇██████████▇█▇▇███████████████████</td></tr><tr><td>f1_none/0</td><td>▂▂▂▂▁▁▅▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>f1_none/1</td><td>▁▁▁▁▇▇██████████████████████████████████</td></tr><tr><td>learning_rate</td><td>▁▁▂▂▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▅▅▆▆▇▇▇█████████████████████████████</td></tr><tr><td>training_loss</td><td>██▇▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▃▃▆▇██▇█▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>validation_loss</td><td>██▇▅▄▅▂▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>0.83926</td></tr><tr><td>f1_none/0</td><td>0.88205</td></tr><tr><td>f1_none/1</td><td>0.79646</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_accuracy</td><td>0.89088</td></tr><tr><td>training_loss</td><td>0.28431</td></tr><tr><td>val_accuracy</td><td>0.85065</td></tr><tr><td>validation_loss</td><td>0.43181</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-serenity-16</strong> at: <a href='https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-16' target=\"_blank\">https://wandb.ai/mhrnciar/basic-nn-torch/runs/vivid-serenity-16</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230324_083725-vivid-serenity-16/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/model.pth\")\n",
    "wandb.save('runs/pima_run_2023-03-22')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8506493506493507"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_pred = model(data)\n",
    "        predictions.append((y_pred > 0.5).int().item())\n",
    "\n",
    "score = accuracy_score(y_test, predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
