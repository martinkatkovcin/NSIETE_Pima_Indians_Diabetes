{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "Using mps device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(torch.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and train-test split\n",
    "\n",
    "First, we load cleaned data and created two datasets - labels (y) and predictors (X) - which were further split into train (80%) and test (20%) sets. To ensure the split will always be the same, we also set the random state seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.314928</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.171779</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.104294</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.185096</td>\n",
       "      <td>0.509202</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.367742</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.445652</td>\n",
       "      <td>0.199519</td>\n",
       "      <td>0.300613</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.503226</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.380368</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.496774</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.117788</td>\n",
       "      <td>0.163599</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.529032</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.243354</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.316129</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.249489</td>\n",
       "      <td>0.101196</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0       0.352941  0.670968       0.489796       0.304348  0.186899  0.314928   \n",
       "1       0.058824  0.264516       0.428571       0.239130  0.106370  0.171779   \n",
       "2       0.470588  0.896774       0.408163       0.271739  0.186899  0.104294   \n",
       "3       0.058824  0.290323       0.428571       0.173913  0.096154  0.202454   \n",
       "4       0.000000  0.600000       0.163265       0.304348  0.185096  0.509202   \n",
       "..           ...       ...            ...            ...       ...       ...   \n",
       "763     0.588235  0.367742       0.530612       0.445652  0.199519  0.300613   \n",
       "764     0.117647  0.503226       0.469388       0.217391  0.106370  0.380368   \n",
       "765     0.294118  0.496774       0.489796       0.173913  0.117788  0.163599   \n",
       "766     0.058824  0.529032       0.367347       0.271739  0.186899  0.243354   \n",
       "767     0.058824  0.316129       0.469388       0.260870  0.106370  0.249489   \n",
       "\n",
       "     DiabetesPedigreeFunction       Age  Outcome  \n",
       "0                    0.234415  0.483333        1  \n",
       "1                    0.116567  0.166667        0  \n",
       "2                    0.253629  0.183333        1  \n",
       "3                    0.038002  0.000000        0  \n",
       "4                    0.943638  0.200000        1  \n",
       "..                        ...       ...      ...  \n",
       "763                  0.039710  0.700000        0  \n",
       "764                  0.111870  0.100000        0  \n",
       "765                  0.071307  0.150000        0  \n",
       "766                  0.115713  0.433333        1  \n",
       "767                  0.101196  0.033333        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/cleaned.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('Outcome', axis=1).values, df.Outcome.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=42)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "y_test = torch.FloatTensor(y_test).unsqueeze(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network consists of input layer, one hidden and an output layer. All layers except output have ReLU activation function and the output layer has Sigmoid activation function, because we need the output to be in interval <0, 1>. Because we are doing binary classification, we used binary cross entropy loss (BCELoss) and as an optimizer we used Adam.\n",
    "\n",
    "Additionally, learning rate scheduler has been used to make the learning rate variable - higher at the beginning of the training and getting gradually smaller as we are nearing the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (f_connected1): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (f_connected2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (out): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:onecycle-cos-deeper) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>▁▁▁▁▁▁▁▁▅▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>f1_none/0</td><td>▁▁▁▁▁▁▁▁▄▄▄▄▃▄▄▆▆▆▇▇▇███████████████████</td></tr><tr><td>f1_none/1</td><td>▁▁▁▁▁▁▁▁▆▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>learning_rate</td><td>▁▁▂▂▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▃▅▅▅▅▅▆▆▆▇▇█████████████████████</td></tr><tr><td>training_loss</td><td>██████▇▇▆▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▅▆▅▅▅▅▆▇▇▇██████████████████████</td></tr><tr><td>validation_loss</td><td>█████▇▇▆▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>0.83405</td></tr><tr><td>f1_none/0</td><td>0.875</td></tr><tr><td>f1_none/1</td><td>0.7931</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_accuracy</td><td>0.88925</td></tr><tr><td>training_loss</td><td>0.28607</td></tr><tr><td>val_accuracy</td><td>0.84416</td></tr><tr><td>validation_loss</td><td>0.42198</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">onecycle-cos-deeper</strong> at: <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-cos-deeper' target=\"_blank\">https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-cos-deeper</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230328_161546-onecycle-cos-deeper/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:onecycle-cos-deeper). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mhrnciar/Desktop/School/FIIT/Ing/2. Semester/Neurónové siete/Cvičenia/basic-nn/wandb/run-20230328_161704-onecycle-cos-1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-cos-1' target=\"_blank\">onecycle-cos-1</a></strong> to <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch' target=\"_blank\">https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-cos-1' target=\"_blank\">https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-cos-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=8, hidden2=4, out_features=1):\n",
    "        super().__init__()\n",
    "        self.f_connected1 = nn.Linear(input_features, hidden1)\n",
    "        self.f_connected2 = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.f_connected1(x))\n",
    "        x = self.relu(self.f_connected2(x))\n",
    "        x = self.sigmoid(self.out(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['torch'].getfloat('start_lr'))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=config['torch'].getfloat('max_lr'),\n",
    "                                                #base_lr=config['torch'].getfloat('start_lr'),\n",
    "                                                epochs=config['default'].getint('epochs'),\n",
    "                                                steps_per_epoch=1,\n",
    "                                                anneal_strategy=config['torch']['strategy'],\n",
    "                                                cycle_momentum=False)\n",
    "\n",
    "run = wandb.init(project=\"basic-nn-torch\", id=\"onecycle-cos-1\")\n",
    "wandb.config.update(config)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training is calculated loss and accuracy and the learning rate is adjusted by the scheduler. During testing is calculated the testing loss and accuracy, as well as F1-score, and all metrics are logged to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "----------------------------------------\n",
      "Training loss: 0.7008240222930908, validation loss: 0.6999808549880981\n",
      "\n",
      "Epoch 20\n",
      "----------------------------------------\n",
      "Training loss: 0.6997541785240173, validation loss: 0.6990656852722168\n",
      "\n",
      "Epoch 30\n",
      "----------------------------------------\n",
      "Training loss: 0.698097825050354, validation loss: 0.6974795460700989\n",
      "\n",
      "Epoch 40\n",
      "----------------------------------------\n",
      "Training loss: 0.6955265998840332, validation loss: 0.6948810815811157\n",
      "\n",
      "Epoch 50\n",
      "----------------------------------------\n",
      "Training loss: 0.6917133927345276, validation loss: 0.6909738779067993\n",
      "\n",
      "Epoch 60\n",
      "----------------------------------------\n",
      "Training loss: 0.686138391494751, validation loss: 0.6852716207504272\n",
      "\n",
      "Epoch 70\n",
      "----------------------------------------\n",
      "Training loss: 0.6774418354034424, validation loss: 0.6754773855209351\n",
      "\n",
      "Epoch 80\n",
      "----------------------------------------\n",
      "Training loss: 0.6470667719841003, validation loss: 0.6417847871780396\n",
      "\n",
      "Epoch 90\n",
      "----------------------------------------\n",
      "Training loss: 0.5981742739677429, validation loss: 0.5941699147224426\n",
      "\n",
      "Epoch 100\n",
      "----------------------------------------\n",
      "Training loss: 0.5162461400032043, validation loss: 0.5194199085235596\n",
      "\n",
      "Epoch 110\n",
      "----------------------------------------\n",
      "Training loss: 0.4604838490486145, validation loss: 0.4790245592594147\n",
      "\n",
      "Epoch 120\n",
      "----------------------------------------\n",
      "Training loss: 0.4400130808353424, validation loss: 0.4580460488796234\n",
      "\n",
      "Epoch 130\n",
      "----------------------------------------\n",
      "Training loss: 0.43045905232429504, validation loss: 0.4575249254703522\n",
      "\n",
      "Epoch 140\n",
      "----------------------------------------\n",
      "Training loss: 0.4214138686656952, validation loss: 0.4527730643749237\n",
      "\n",
      "Epoch 150\n",
      "----------------------------------------\n",
      "Training loss: 0.4104693830013275, validation loss: 0.44894343614578247\n",
      "\n",
      "Epoch 160\n",
      "----------------------------------------\n",
      "Training loss: 0.40062984824180603, validation loss: 0.43935537338256836\n",
      "\n",
      "Epoch 170\n",
      "----------------------------------------\n",
      "Training loss: 0.3907776474952698, validation loss: 0.43119004368782043\n",
      "\n",
      "Epoch 180\n",
      "----------------------------------------\n",
      "Training loss: 0.38185733556747437, validation loss: 0.42829272150993347\n",
      "\n",
      "Epoch 190\n",
      "----------------------------------------\n",
      "Training loss: 0.3733622431755066, validation loss: 0.4288193881511688\n",
      "\n",
      "Epoch 200\n",
      "----------------------------------------\n",
      "Training loss: 0.3648965060710907, validation loss: 0.42662107944488525\n",
      "\n",
      "Epoch 210\n",
      "----------------------------------------\n",
      "Training loss: 0.3551301658153534, validation loss: 0.42384859919548035\n",
      "\n",
      "Epoch 220\n",
      "----------------------------------------\n",
      "Training loss: 0.3444505035877228, validation loss: 0.41520512104034424\n",
      "\n",
      "Epoch 230\n",
      "----------------------------------------\n",
      "Training loss: 0.3343147933483124, validation loss: 0.41480588912963867\n",
      "\n",
      "Epoch 240\n",
      "----------------------------------------\n",
      "Training loss: 0.3265988528728485, validation loss: 0.4102371633052826\n",
      "\n",
      "Epoch 250\n",
      "----------------------------------------\n",
      "Training loss: 0.3205001950263977, validation loss: 0.409625381231308\n",
      "\n",
      "Epoch 260\n",
      "----------------------------------------\n",
      "Training loss: 0.3155205547809601, validation loss: 0.40974152088165283\n",
      "\n",
      "Epoch 270\n",
      "----------------------------------------\n",
      "Training loss: 0.3112378418445587, validation loss: 0.4112185835838318\n",
      "\n",
      "Epoch 280\n",
      "----------------------------------------\n",
      "Training loss: 0.30774688720703125, validation loss: 0.41290125250816345\n",
      "\n",
      "Epoch 290\n",
      "----------------------------------------\n",
      "Training loss: 0.30492904782295227, validation loss: 0.41483569145202637\n",
      "\n",
      "Epoch 300\n",
      "----------------------------------------\n",
      "Training loss: 0.3026694059371948, validation loss: 0.4137170910835266\n",
      "\n",
      "Epoch 310\n",
      "----------------------------------------\n",
      "Training loss: 0.3009042739868164, validation loss: 0.4178178012371063\n",
      "\n",
      "Epoch 320\n",
      "----------------------------------------\n",
      "Training loss: 0.2995092570781708, validation loss: 0.41698768734931946\n",
      "\n",
      "Epoch 330\n",
      "----------------------------------------\n",
      "Training loss: 0.2983018755912781, validation loss: 0.4171292185783386\n",
      "\n",
      "Epoch 340\n",
      "----------------------------------------\n",
      "Training loss: 0.2973199784755707, validation loss: 0.41872891783714294\n",
      "\n",
      "Epoch 350\n",
      "----------------------------------------\n",
      "Training loss: 0.2965613305568695, validation loss: 0.41837751865386963\n",
      "\n",
      "Epoch 360\n",
      "----------------------------------------\n",
      "Training loss: 0.2959819734096527, validation loss: 0.4191122055053711\n",
      "\n",
      "Epoch 370\n",
      "----------------------------------------\n",
      "Training loss: 0.29552435874938965, validation loss: 0.42013123631477356\n",
      "\n",
      "Epoch 380\n",
      "----------------------------------------\n",
      "Training loss: 0.29514873027801514, validation loss: 0.4217037260532379\n",
      "\n",
      "Epoch 390\n",
      "----------------------------------------\n",
      "Training loss: 0.29485124349594116, validation loss: 0.4213811159133911\n",
      "\n",
      "Epoch 400\n",
      "----------------------------------------\n",
      "Training loss: 0.29461681842803955, validation loss: 0.4216832220554352\n",
      "\n",
      "Epoch 410\n",
      "----------------------------------------\n",
      "Training loss: 0.2944120764732361, validation loss: 0.42217397689819336\n",
      "\n",
      "Epoch 420\n",
      "----------------------------------------\n",
      "Training loss: 0.29423201084136963, validation loss: 0.4223804771900177\n",
      "\n",
      "Epoch 430\n",
      "----------------------------------------\n",
      "Training loss: 0.2940848469734192, validation loss: 0.4229860305786133\n",
      "\n",
      "Epoch 440\n",
      "----------------------------------------\n",
      "Training loss: 0.29397106170654297, validation loss: 0.42313700914382935\n",
      "\n",
      "Epoch 450\n",
      "----------------------------------------\n",
      "Training loss: 0.2938956320285797, validation loss: 0.4233565926551819\n",
      "\n",
      "Epoch 460\n",
      "----------------------------------------\n",
      "Training loss: 0.29384371638298035, validation loss: 0.4234231114387512\n",
      "\n",
      "Epoch 470\n",
      "----------------------------------------\n",
      "Training loss: 0.2938145101070404, validation loss: 0.4236200153827667\n",
      "\n",
      "Epoch 480\n",
      "----------------------------------------\n",
      "Training loss: 0.2937963902950287, validation loss: 0.42359399795532227\n",
      "\n",
      "Epoch 490\n",
      "----------------------------------------\n",
      "Training loss: 0.29378995299339294, validation loss: 0.42359593510627747\n",
      "\n",
      "Epoch 500\n",
      "----------------------------------------\n",
      "Training loss: 0.2937886118888855, validation loss: 0.4235830008983612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "\n",
    "for i in range(config['default'].getint('epochs')):\n",
    "    i += 1\n",
    "    y_pred = model.forward(X_train)\n",
    "    train_loss = loss_fn(y_pred, y_train)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    y_pred = (y_pred > 0.5).int()\n",
    "    train_accuracy = accuracy_score(y_train.squeeze(1).int(), y_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "    scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        val_loss = loss_fn(y_pred, y_test)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        y_pred = (y_pred > 0.5).int()\n",
    "        wandb.log({'training_loss': train_loss, 'validation_loss': val_loss}, commit=False)\n",
    "\n",
    "        f1_none = f1_score(y_test.squeeze(1).int(), y_pred, average=None)\n",
    "        f1_none = {'f1_none/' + str(e): v for e,v in enumerate(f1_none)}\n",
    "        wandb.log(f1_none, commit=False)\n",
    "\n",
    "        f1_macro = f1_score(y_test.squeeze(1).int(), y_pred, average='macro')\n",
    "        wandb.log({'f1_macro': f1_macro}, commit=False)\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_test.squeeze(1).int(), y_pred)\n",
    "        wandb.log({'train_accuracy': train_accuracy, 'val_accuracy': val_accuracy})\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}')\n",
    "        print('-' * 40)\n",
    "        print(f'Training loss: {train_loss}, validation loss: {val_loss}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>▁▁▁▁▄▃▃▄▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>f1_none/0</td><td>▁▁▁▁▇▇▇▇████▇▇██████████████████████████</td></tr><tr><td>f1_none/1</td><td>▅▅▅▅▂▁▁▃▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>learning_rate</td><td>▁▁▂▂▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>training_loss</td><td>██████▇▆▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▅▅▅▅▇▇▇▇▇▇▇▇▇█▇█████████████████████</td></tr><tr><td>validation_loss</td><td>██████▇▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_macro</td><td>0.83405</td></tr><tr><td>f1_none/0</td><td>0.875</td></tr><tr><td>f1_none/1</td><td>0.7931</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_accuracy</td><td>0.88436</td></tr><tr><td>training_loss</td><td>0.29379</td></tr><tr><td>val_accuracy</td><td>0.84416</td></tr><tr><td>validation_loss</td><td>0.42358</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">onecycle-cos-1</strong> at: <a href='https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-cos-1' target=\"_blank\">https://wandb.ai/nsiete-hrnciar-katkovcin/basic-nn-torch/runs/onecycle-cos-1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230328_161704-onecycle-cos-1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/model.pth\")\n",
    "wandb.save('runs/pima_run_2023-03-22')\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of training, we can calculate the final testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441558441558441"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_pred = model(data)\n",
    "        predictions.append((y_pred > 0.5).int().item())\n",
    "\n",
    "score = accuracy_score(y_test, predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
